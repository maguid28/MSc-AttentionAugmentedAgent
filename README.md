# Master's Dissertation: Investigating the Impact of Entropy Regularisation on an Attention Augmented Agent

## Overview
This repository contains the research conducted for my master's dissertation, which investigates the impact of entropy regularisation on reinforcement learning (RL) agents. The project aims to attempt to identify if there is the optimal entropy coefficients that maximize learning and stability in three different Atari environments: Space Invaders, Seaquest, and Breakout.

## Abstract
In reinforcement learning, the balance between exploration and exploitation is crucial for effective training. Entropy regularisation is a technique used to promote exploration and prevent the convergence to sub-optimal policies by introducing stochasticity in policy distributions. This research focuses on how varying entropy regularisation coefficients affect the learning outcomes and stability of an RL agent augmented with an attention module. Through experimentation with various entropy coefficients and decay strategies, it was found that decaying entropy typically offers better performance than static entropy settings. Additionally, the complexity added by the attention network was evaluated to determine its impact on the agent’s learning capabilities. The findings indicate a correlation between increased network complexity and the degradation of learning efficiency.

## Key Experiments
- **Entropy Coefficients**: Various coefficients were tested to determine which configurations yield the best results in terms of learning performance and stability.
- **Decay Strategies**: Both static and dynamic entropy configurations were explored, with particular focus on understanding their impact across different game environments.
- **Attention Mechanism**: The attention-augmented agent's decision-making processes were visualised using attention maps and provides insight into the influence of entropy regularisation on attention patterns.

## Results and Conclusions

### The Impact of Increasing the Complexity of an RL Model

In the experiments, five models were tested in each of the three Atari environments and results are shown in Figures 4, 5, 6, 7 and 8. Each of these figures compare five implemented models, consisting of: Duarte et al’s implementation, which is the Dynamic Heads Paper Model; the standard IMPALA Model without any modifications to the architecture of the model, the IMPALA model with the Policy LSTM included; the IMPALA with Policy LSTM and Convolutional LSTM; and the final IMPALA Attention Model, which includes both the policy and convolutional LSTMs and the attention network. From examining Figures 4, 5, 6, 7 and 8, we can see some trends emerging in all three environments. The first noticeable observation is that the Dynamic Heads Paper Model implementation did not learn any of the environments. This is valuable to display as we can use this as a lower bound of what a model which is not learning the environment looks like. Another observation is that the IMPALA Model performs best in all three environments, which is not surprising, as this is a state-of- the-art model and is often used in RL tasks. This can be seen as our upper bound of what a model which is learning the environment looks like. The IMPALA with Policy LSTM model adds an extra layer of complexity to the unmodified IMPALA Model, and from examining performance across all three Atari environments, we can see that there is a slight drop in performance. The remaining models follow this trend of decreased performance correlating to model complexity, as we can see this occurring following the introduction of the policy and convolutional LSTMs to the IMPALA Model, and even more significantly across all
29
environments with the introduction of the attention network. This observed degradation of learning performance across all environments aligns to (Sinha et al., 2020), where they identify that increasing the number of layers in a neural network generally does not improve performance in RL tasks; conversely it often leads to a decrease in performance as the depth of a network is increased. Each of the additions to the IMPALA architecture is increasing the depth of the network, and what results is a visible degradation of network performance and cumulative reward across all three of the Atari environments. Although there is a decrease in model performance, learning is still observed in all scenarios other than the Dynamic Heads Paper Model. Due to the higher performing models shown in Figure 8, it is difficult to see whether learning is occurring in the IMPALA Attention Model, so Figure 10 isolates the IMPALA Attention Model and the Dynamic Heads Paper Model to allow for a more granular comparison. Although learning is quite poor in the IMPALA Attention Model, the cumulative reward is still increasing over time, albeit very slowly. The only variation between the Dynamic Heads Paper Model compared to the IMPALA Attention Model is the use of the IMPALA CNN as part of the vision core instead of the CNN implementation described in (Duarte et al., 2024). A reason for the improved performance when using the IMPALA CNN could be due to the fact that it uses residual blocks (He et al., 2015) which facilitate networks to go deeper due to its use of concatenating the identity of the input to the output of each residual block. The key observation here is that although learning in the IMPALA Attention Model is hindered by increased complexity, it is still occurring. This model is used for all subsequent experiments.

### Varying Entropy Coefficients on an Attention Augmented Agent

From examining Figures 11, 13 and 15 there are several observations that become apparent as a result of running an agent configured with various entropy coefficients three times and comparing the average cumulative rewards over time. The first observation is that generally the decaying entropy coefficients perform consistently better than the static entropy coefficients across all environments. In Breakout, the Decay 0.05 – 0.01 results in the best performance, and the Decay 0.1 – 0.01 also marginally outperforms the static 0.01 entropy coefficient by end of training. A similar observation can be drawn from the average rewards over time for the SpaceInvaders environment, shown in Figure 13, where the Decay 0.05 – 0.01 results in the highest cumulative reward by the end of training, with the Decay 0.1 – 0.01 following closely. In the average results for Seaquest, Figure 15, it can be clearly observed that the Decay 0.1 – 0.01 performs noticeably better than any of the other entropy coefficients. This indicates that the using an entropy decay in these experiments can improve stability which could be due to the more controlled decrease in entropy over the course of training, which can be observed in detail in Figures 20, 21 and 22.
Other than the notable performance of the decaying entropy coefficients, it is worth mentioning that the static 0.05 coefficient also showed promising results. In Figure 13 we can see that static 0.05 coefficient marginally outperforms the Decay 0.1 – 0.01 for SpaceInvaders. Furthermore, in Figure 15, we can see that Seaquest also had considerable performance for the static 0.05 coefficient, as it achieved higher average cumulative reward over time than the Decay 0.05 – 0.01. Interestingly, in Figure 11 for Breakout, an environment considered relatively less complex, the static 0.05 coefficient was one of the worst performing coefficients.
There is also a benefit to running RL agents multiple times due to their inherent stochasticity, which can result in different learning trajectories without the adjustment of any hyperparameters. Due to this constraint, it is necessary to run a model multiple times and observe the best performing agent to have a better understanding of the upper bounds of the
35
agent’s capabilities given a set of hyperparameters. Observing the best iterative run of the models across the three environments in Figures 12, 14 and 16 reveal additional insights into each agent’s performance. In the literature, an entropy coefficient of 0.01 is often used (Mott et al., 2019; Duarte et al., 2024). If we take this as a baseline, we can see that in all three environments, an entropy coefficient of 0.01 results in relatively decent performance, potentially justifying why this entropy coefficient value is commonly used. It can also be observed that in all three environments, for both the average and best performances, the Decay 0.05 – 0.01 consistently results in a higher cumulative reward than the static coefficient of 0.01 by the end of training, albeit marginal in all three cases.
Comparing the average runs and best runs for the various entropy coefficients can provide further insight. For example, while some of the best runs of the static entropy coefficients resulted in high cumulative reward, it can also be observed that the average performance of these entropy coefficients did not demonstrate the same results, especially when compared to the average of the decaying coefficients, i.e. 0.001 in Breakout, 0.01 in SpaceInvaders, and 0.01 in Seaquest. This indicates that the performance of the agent is less stable i.e. higher variance between runs, in the static coefficient agents. The individual agent runs configured with the various entropy coefficients are depicted for Breakout, SpaceInvaders and Seaquest in Figures 17, 18 and 19 respectively for further reference as to how the average and best runs were determined.

### Discussion on the Increased Interpretability Facilitated by Attention Maps

The use an attention network in the RL agent was heavily motivated by the notion of interpretability which can be achieved with the integration of attention map visualisation logic. Figures 26, 28 and 30 show a snapshot of the attention maps for the untrained IMPALA Attention Model in the Breakout, SpaceInvaders and Seaquest environments respectively. For comparison, Figures 27, 29 and 31 portray a snapshot of the attention maps for the same IMPALA attention agent after it has been trained on the three Atari environments for 10 million frames each. These snapshots provide qualitative information on what the agent is attending to, however a more appropriate way to view these maps is through video, which is an undeniably better format to draw qualitative observations from when working with spatiotemporal sequences of data. The following observations are derived from the video which can be viewed from [here](https://www.youtube.com/watch?v=hS4bjPz-kGw)
When examining the untrained SpaceInvaders agent, it is apparent that the agent is not focusing at all on its own position in the environment and instead is sporadically focusing on different sections of the enemy ship area. These points of focus seem to be random, as the areas it is focusing on do not have an impact on the immediate threat of getting destroyed. It is difficult to draw too many qualitative observations from the untrained agent other than to observe where the agent should perhaps be paying attention to, to achieve the goal of surviving in the environment and accumulating reward. As seen in the video and in Figure 28 below, attention head 4 is not active at all in this case and does not contribute. Alternatively, the trained SpaceInvaders model in the video is focusing on its own position in the environment and can be observed in attention head 1, 2 and 3, with head 4 seeming to attend to the area where it is considering navigating towards. We can also observe later in the video that head 1 and 2 seem to be attending to the immediate threat while 3 is focusing on other areas where the enemy space ships are present and approaching. Scans of the environment seem to be occurring sporadically in head 1 and head 4 as well, which may be a mechanism that the agent employs to scan for other regions of interest. There is some overlap in what each head is paying attention to – which can bring up the question: are 4 heads actually necessary in an environment of this complexity?

Examining the untrained Seaquest agent, it is apparent that the agent already is focusing on its own position in the environment, visible in attention head 2 and 3. This was surprising as this was something that was believed to be a learned behaviour, but the transition of pixel values could also be a contributor to the attention heads focus. Head 1 seems to be attending to scans of the environment, but again it is difficult to draw many qualitative observations from the untrained model. They are mainly used as a comparison for the trained models to see what has changed. After the model has been trained, we can observe that attention head 1 is attending to the agent’s position in the environment almost exclusively. Attention heads 2 and 3 are focusing on almost identical regions at each timestep, and head 4 is not adding too much value as it is attending to regions of the environment which are not important.
Examining the untrained Breakout agent, the first clear observation is that attention head 4 is not active at all. It can also be observed that heads 1 and 3 are focusing on regions of the environment which are currently inaccessible and there is no value in attending to them. There are brief sequences of timesteps where we can observe head 2 and 3 focusing on the ball as well as its own position in the environment. After training it can be observed that much of the attention of heads 2, 3 and 4 are now focused on the position of the ball as well as sharing responsibility of attending to the agents position. We can also see that attention head 1 is initially focused on an area where it already has made a tunnel in the blocks, but as the environment switches to a different trajectory its focus changes to be on an unimportant region of the environment, the game score. (Mott et al., 2019) observed that given enough time, the agent uses attention to set up “tripwires” to create an alert when an object of interest passes it, indicating to the agent that it needs to perform some action. While in this scenario attention head 1, and occasionally the other attention heads, seem to land their focus on the game score, which could not indicate to the agent that an action is needed, perhaps it could be connecting the score to reward, and may be following a similar path of deduction as the “tripwire” behaviour described. Mott et al trained their agent for 1 billion frames compared to these experiments of 10 million frames, so attention may become more focused or specialised when given enough time.
Another observation which is present generally across all environments is how active each attention head is over time. Generally, we can see from Figures 23, 24, and 25 that the activity of each attention head is varies greatly at the start of training, but as training progresses it generally can be observed that all heads become more or less equally active as their entropy starts to stabilise. Some notable exceptions to this observation are for the static entropy coefficients of 0.1 in Seaquest, present in Figure 25, and 0.1 in SpaceInvaders, visible in Figure 24. This could be due to 0.1 being too high of an entropy coefficient, resulting in poor convergence.

### Breakout Comparative Average Reward with Varying Entropy Coefficents

The comparative average rewards with varying entropy coefficients in the Breakout environment:

![Breakout Comparative Average Reward](https://github.com/maguid28/MSc-AttentionAugmentedRL/blob/main/results/Compiled%20Experiment%20Results/Breakout%20Comparative%20Average%20Reward%20with%20Varying%20Entropy%20Coefficents.png "Breakout Comparative Average Reward with Varying Entropy Coefficients")


## Video Demonstration
A video demonstration of the visualized attention maps and the agent's performance across different settings and environments can be viewed [here](https://www.youtube.com/watch?v=hS4bjPz-kGw).


## Keywords
`Reinforcement Learning`, `Entropy Regularisation`, `Exploration-Exploitation Balance`, `Attention`, `Attention Maps`, `Hyperparameter Tuning`

## Citation
If you find this research useful or refer to it in your academic or professional work, please consider citing it.

Maguire, D. (2024). Master's Dissertation: Investigating the Impact of Entropy Regularisation on an Attention Augmented Agent. [Source code]. https://github.com/maguid28/MSc-AttentionAugmentedRL


