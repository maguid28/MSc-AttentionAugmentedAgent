{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SVVUGbBVBtBp"
      },
      "source": [
        "# Investigating the Impact of Entropy Regularisation on an Attention Augmented Agent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KEFTZsWaCBOi"
      },
      "source": [
        "Author: Daniel Maguire"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YjA0WF41vyE3"
      },
      "source": [
        "The code used to create these experiments is built upon the following implementations:\n",
        "\n",
        "**Attention Augmented RL Agent**\n",
        "https://github.com/cjlovering/Towards-Interpretable-Reinforcement-Learning-Using-Attention-Augmented-Agents-Replication\n",
        "\n",
        "\n",
        "**Attention Map Visualisation**\n",
        "https://github.com/greydanus/visualize_atari\n",
        "\n",
        "\n",
        "**A2C PPO Working Implementation**\n",
        "https://github.com/rgilman33/simple-A2C-PPO\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hFD49xghlgXJ"
      },
      "source": [
        "#Experiment Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy5MNmWvmAYP"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"Attention_BR_10m_steps_ent_decay_0.01_to_0.001_RUN1\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RJiRWaUSv3lR"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wun0V_D9D_qK"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VShkM-fEFYP"
      },
      "outputs": [],
      "source": [
        "#Number of trajectories/synchronous workers learning at the same time\n",
        "n_envs = 72\n",
        "#Number of steps in each trajectory\n",
        "n_steps = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztPWR29OnYn4"
      },
      "outputs": [],
      "source": [
        "is_dynamic_heads_model = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPpSo9MvBgPn"
      },
      "outputs": [],
      "source": [
        "save_attention_map_before_training = True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MENvMezhFnL5"
      },
      "source": [
        "### Environment Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raoADF-_FqR-"
      },
      "outputs": [],
      "source": [
        "env_name = 'Breakout-v4'\n",
        "#env_name = 'Seaquest-v4'\n",
        "#env_name = 'SpaceInvaders-v4'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b-c7E-HPFsRb"
      },
      "source": [
        "### Entropy Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxGHvyp4o4Ou"
      },
      "outputs": [],
      "source": [
        "entropy_coef = 0.01 #Constant for entropy if decay is set to False\n",
        "\n",
        "enable_entropy_decay = True\n",
        "entropy_coef_start = 0.01  #Starting value of entropy\n",
        "entropy_coef_end = 0.001   #Ending value of entropy\n",
        "entropy_decay = 0.995     #Decay rate of entropy\n",
        "\n",
        "entropy_decay_type='logarithmic'  #'exponential', 'logarithmic', 'stepwise'\n",
        "step_decay_interval=50  #interval for stepwise decay,\n",
        "custom_filename = \"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5ynIif5SGVt4"
      },
      "source": [
        "### Training Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiJ2du7TGU6y",
        "outputId": "589c0a97-6dec-4d3a-9872-cf08059f87a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(18432, 1, 2304)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "if is_dynamic_heads_model:\n",
        "  lr = 1e-4\n",
        "  n_opt_epochs = 1 #Adjusting to 1 as Dynamic heads paper uses A2C which does not make use of opt epochs\n",
        "else:\n",
        "  lr = 5e-4 #Found that this was a decent learning rate for these experiments\n",
        "  n_opt_epochs = 3\n",
        "\n",
        "n_obs_per_round = n_envs * n_steps;\n",
        "#num of steps\n",
        "target_n_obs = 20000\n",
        "n_minibatches = 8\n",
        "bs = n_obs_per_round // n_minibatches\n",
        "n_rounds = target_n_obs // n_obs_per_round\n",
        "\n",
        "save_attention_map_interval = 3_000_000\n",
        "\n",
        "accumulation_steps = 1\n",
        "\n",
        "value_loss_coef = .5\n",
        "\n",
        "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "logs_dir_path = \"log/log_\" + str(env_name) + \"_\" + str(current_time)\n",
        "if not os.path.exists(logs_dir_path):\n",
        "    os.makedirs(logs_dir_path)  #Creates log directory\n",
        "\n",
        "n_obs_per_round, n_rounds, bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXbbM7qFuOrD"
      },
      "outputs": [],
      "source": [
        "attention_head_1, attention_head_2, attention_head_3, attention_head_4 =[],[],[],[]\n",
        "r2_scores, entropies, entropy_coefs, value_losses, policy_losses, a2c_losses, value_targets_list, value_predictions_list = [], [], [], [], [], [], [], []\n",
        "history_hx, history_cx = [], []"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q07TEt0WHs-p"
      },
      "source": [
        "### Logging Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC3pJUsuHsnC"
      },
      "outputs": [],
      "source": [
        "print_logs = False\n",
        "print_model_logs = False\n",
        "print_saliency_maps = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sSK8Cduq4c_Y"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAl0t7Rg4i00"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import RMSprop\n",
        "from torch.optim import Adam\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score\n",
        "import tempfile\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from matplotlib import colormaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vl5mRc-4n7h",
        "outputId": "abc21016-2d1f-4eaf-b035-961ce044e60d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchlens\n",
            "  Downloading torchlens-0.1.18-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchlens) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from torchlens) (2.1.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from torchlens) (7.4.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchlens) (4.66.5)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from torchlens) (7.34.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchlens) (0.20.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython->torchlens)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->torchlens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchlens) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchlens) (2024.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->torchlens) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->torchlens) (24.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->torchlens) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->torchlens) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->torchlens) (2.0.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->torchlens) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->torchlens) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->torchlens) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->torchlens) (1.16.0)\n",
            "Downloading torchlens-0.1.18-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: jedi, torchlens\n",
            "Successfully installed jedi-0.19.1 torchlens-0.1.18\n"
          ]
        }
      ],
      "source": [
        "!pip install torchlens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVC5JqFtFSBP",
        "outputId": "54e45fdf-b237-4cb4-a7d1-0de6e7cd160b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 7,813 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.11 [28.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.11 [863 kB]\n",
            "Fetched 7,813 kB in 3s (2,785 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 123595 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.11_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.11_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hStarting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !apt-get install -y xvfb\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/xvfb -O ../xvfb\n",
        "    !pip install -q gymnasium[atari,accept-rom-license]\n",
        "    !pip install -q tensorboardX\n",
        "    !touch .setup_complete\n",
        "\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVAvJ6suF9di",
        "outputId": "de4eb048-49cd-4635-af0a-4f28b97bda91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.3.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n",
            "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable_baselines3\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 stable_baselines3-2.3.2\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from stable_baselines3.common.env_util import make_vec_env\n",
        "except:\n",
        "    !pip install stable_baselines3\n",
        "    from stable_baselines3.common.env_util import make_vec_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88kRzBAP9Jk_",
        "outputId": "088b456a-10c3-4dfa-e14b-c19e0c559125"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "log_file = os.path.join(logs_dir_path, \"training_log.csv\")\n",
        "\n",
        "if not os.path.exists(logs_dir_path):\n",
        "    os.makedirs(logs_dir_path)\n",
        "\n",
        "with open(log_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Step\", \"Entropy\", \"Policy Loss\", \"Value Loss\", \"A2C Loss\", \"Value Target\", \"Value Prediction\", \"Reward\", \"Loss\"])\n",
        "\n",
        "def log_to_csv(step, entropy, policy_loss, value_loss, a2c_loss, value_target, value_prediction, avg_score=None, loss=None):\n",
        "    with open(log_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        row = [step, entropy, policy_loss, value_loss, a2c_loss, value_target, value_prediction, avg_score, loss]\n",
        "        writer.writerow(row)\n",
        "\n",
        "def update_last_csv_line(avg_score, loss):\n",
        "    with open(log_file, mode='r') as file:\n",
        "        rows = list(csv.reader(file))\n",
        "\n",
        "    if rows:\n",
        "        rows[-1][-2] = avg_score\n",
        "        rows[-1][-1] = loss\n",
        "\n",
        "    with open(log_file, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerows(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoHncnzFE3sh",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4Ju-bB2FLFn",
        "outputId": "caa2628d-af8d-4bcb-b0a9-0e6a37f5f784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting procgen\n",
            "  Downloading procgen-0.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from procgen) (1.26.4)\n",
            "Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from procgen) (0.25.2)\n",
            "Collecting gym3<1.0.0,>=0.3.3 (from procgen)\n",
            "  Downloading gym3-0.3.3-py3-none-any.whl.metadata (835 bytes)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from procgen) (3.15.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (0.0.8)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.17.0)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (2.34.2)\n",
            "Collecting imageio-ffmpeg<0.4.0,>=0.3.0 (from gym3<1.0.0,>=0.3.3->procgen)\n",
            "  Downloading imageio_ffmpeg-0.3.0-py3-none-manylinux2010_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting glfw<2.0.0,>=1.8.6 (from gym3<1.0.0,>=0.3.3->procgen)\n",
            "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting moderngl<6.0.0,>=5.5.4 (from gym3<1.0.0,>=0.3.3->procgen)\n",
            "  Downloading moderngl-5.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.22)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (9.4.0)\n",
            "Collecting glcontext>=3.0.0 (from moderngl<6.0.0,>=5.5.4->gym3<1.0.0,>=0.3.3->procgen)\n",
            "  Downloading glcontext-3.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
            "Downloading procgen-0.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gym3-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imageio_ffmpeg-0.3.0-py3-none-manylinux2010_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading moderngl-5.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glcontext-3.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imageio-ffmpeg, glfw, glcontext, moderngl, gym3, procgen\n",
            "  Attempting uninstall: imageio-ffmpeg\n",
            "    Found existing installation: imageio-ffmpeg 0.5.1\n",
            "    Uninstalling imageio-ffmpeg-0.5.1:\n",
            "      Successfully uninstalled imageio-ffmpeg-0.5.1\n",
            "Successfully installed glcontext-3.0.0 glfw-1.12.0 gym3-0.3.3 imageio-ffmpeg-0.3.0 moderngl-5.11.1 procgen-0.10.7\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from procgen import ProcgenEnv\n",
        "except:\n",
        "    !pip install procgen\n",
        "    from procgen import ProcgenEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-CJds8n44ny",
        "outputId": "d1ad27fd-880c-44d9-aff3-0544af7814b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available else 'cpu'; print(device)\n",
        "if device != 'cuda': print(\"No cuda detected.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b6csUrNscgek"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRAvwiUHwJ22",
        "outputId": "94815adf-0217-45bd-df14-7d89a8945a92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Destination Dir: /content/drive/My Drive/20240825_012524_Attention_BR_10m_steps_ent_decay_0.01_to_0.001_RUN1\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import drive\n",
        "from IPython import get_ipython\n",
        "from datetime import datetime\n",
        "\n",
        "!pip install -q ipython\n",
        "def copy_folders_to_drive(src_dir, dest_dir):\n",
        "    if not os.path.exists(dest_dir):\n",
        "        os.makedirs(dest_dir)\n",
        "\n",
        "    for item in os.listdir(src_dir):\n",
        "        src_item_path = os.path.join(src_dir, item)\n",
        "        dest_item_path = os.path.join(dest_dir, item)\n",
        "\n",
        "        if os.path.isdir(src_item_path):\n",
        "            shutil.copytree(src_item_path, dest_item_path)\n",
        "\n",
        "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "src_directory = '/content/log'\n",
        "save_dir_name = current_time + \"_\" + experiment_name\n",
        "dest_directory = f'/content/drive/My Drive/{save_dir_name}'\n",
        "print(f\"Destination Dir: {dest_directory}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9UAnDSmckbc"
      },
      "outputs": [],
      "source": [
        "import contextlib\n",
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "import time\n",
        "from collections import deque\n",
        "import torch\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Copy-pasted from OpenAI to obviate dependency on Baselines. Required for vectorized environments.\n",
        "\"\"\"\n",
        "\n",
        "class AlreadySteppingError(Exception):\n",
        "    \"\"\"\n",
        "    Raised when an asynchronous step is running while\n",
        "    step_async() is called again.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        msg = 'already running an async step'\n",
        "        Exception.__init__(self, msg)\n",
        "\n",
        "\n",
        "class NotSteppingError(Exception):\n",
        "    \"\"\"\n",
        "    Raised when an asynchronous step is not running but\n",
        "    step_wait() is called.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        msg = 'not running an async step'\n",
        "        Exception.__init__(self, msg)\n",
        "\n",
        "\n",
        "class VecEnv(ABC):\n",
        "    \"\"\"\n",
        "    An abstract asynchronous, vectorized environment.\n",
        "    Used to batch data from multiple copies of an environment, so that\n",
        "    each observation becomes an batch of observations, and expected action is a batch of actions to\n",
        "    be applied per-environment.\n",
        "    \"\"\"\n",
        "    closed = False\n",
        "    viewer = None\n",
        "\n",
        "    metadata = {\n",
        "        'render.modes': ['human', 'rgb_array']\n",
        "    }\n",
        "\n",
        "    def __init__(self, num_envs, observation_space, action_space):\n",
        "        self.num_envs = num_envs\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset all the environments and return an array of\n",
        "        observations, or a dict of observation arrays.\n",
        "\n",
        "        If step_async is still doing work, that work will\n",
        "        be cancelled and step_wait() should not be called\n",
        "        until step_async() is invoked again.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step_async(self, actions):\n",
        "        \"\"\"\n",
        "        Tell all the environments to start taking a step\n",
        "        with the given actions.\n",
        "        Call step_wait() to get the results of the step.\n",
        "\n",
        "        You should not call this if a step_async run is\n",
        "        already pending.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step_wait(self):\n",
        "        \"\"\"\n",
        "        Wait for the step taken with step_async().\n",
        "\n",
        "        Returns (obs, rews, dones, infos):\n",
        "         - obs: an array of observations, or a dict of\n",
        "                arrays of observations.\n",
        "         - rews: an array of rewards\n",
        "         - dones: an array of \"episode done\" booleans\n",
        "         - infos: a sequence of info objects\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def close_extras(self):\n",
        "        \"\"\"\n",
        "        Clean up the  extra resources, beyond what's in this base class.\n",
        "        Only runs when not self.closed.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        if self.closed:\n",
        "            return\n",
        "        if self.viewer is not None:\n",
        "            self.viewer.close()\n",
        "        self.close_extras()\n",
        "        self.closed = True\n",
        "\n",
        "    def step(self, actions):\n",
        "        \"\"\"\n",
        "        Step the environments synchronously.\n",
        "\n",
        "        This is available for backwards compatibility.\n",
        "        \"\"\"\n",
        "        self.step_async(actions)\n",
        "        return self.step_wait()\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        imgs = self.get_images()\n",
        "        bigimg = \"ARGHH\" #tile_images(imgs)\n",
        "        if mode == 'human':\n",
        "            self.get_viewer().imshow(bigimg)\n",
        "            return self.get_viewer().isopen\n",
        "        elif mode == 'rgb_array':\n",
        "            return bigimg\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def get_images(self):\n",
        "        \"\"\"\n",
        "        Return RGB images from each environment\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def unwrapped(self):\n",
        "        if isinstance(self, VecEnvWrapper):\n",
        "            return self.venv.unwrapped\n",
        "        else:\n",
        "            return self\n",
        "\n",
        "    def get_viewer(self):\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.SimpleImageViewer()\n",
        "        return self.viewer\n",
        "\n",
        "\n",
        "class VecEnvWrapper(VecEnv):\n",
        "    \"\"\"\n",
        "    An environment wrapper that applies to an entire batch\n",
        "    of environments at once.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, venv, observation_space=None, action_space=None):\n",
        "        self.venv = venv\n",
        "        super().__init__(num_envs=venv.num_envs,\n",
        "                        observation_space=observation_space or venv.observation_space,\n",
        "                        action_space=action_space or venv.action_space)\n",
        "\n",
        "    def step_async(self, actions):\n",
        "        self.venv.step_async(actions)\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step_wait(self):\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        return self.venv.close()\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        return self.venv.render(mode=mode)\n",
        "\n",
        "    def get_images(self):\n",
        "        return self.venv.get_images()\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        if name.startswith('_'):\n",
        "            raise AttributeError(\"attempted to get missing private attribute '{}'\".format(name))\n",
        "        return getattr(self.venv, name)\n",
        "\n",
        "\n",
        "class VecEnvObservationWrapper(VecEnvWrapper):\n",
        "    @abstractmethod\n",
        "    def process(self, obs):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.venv.reset()\n",
        "        return self.process(obs)\n",
        "\n",
        "    def step_wait(self):\n",
        "        obs, rews, dones, infos = self.venv.step_wait()\n",
        "        return self.process(obs), rews, dones, infos\n",
        "\n",
        "\n",
        "class CloudpickleWrapper(object):\n",
        "    \"\"\"\n",
        "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "\n",
        "    def __getstate__(self):\n",
        "        import cloudpickle\n",
        "        return cloudpickle.dumps(self.x)\n",
        "\n",
        "    def __setstate__(self, ob):\n",
        "        import pickle\n",
        "        self.x = pickle.loads(ob)\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def clear_mpi_env_vars():\n",
        "    \"\"\"\n",
        "    from mpi4py import MPI will call MPI_Init by default.  If the child process has MPI environment variables, MPI will think that the child process is an MPI process just like the parent and do bad things such as hang.\n",
        "    This context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing\n",
        "    Processes.\n",
        "    \"\"\"\n",
        "    removed_environment = {}\n",
        "    for k, v in list(os.environ.items()):\n",
        "        for prefix in ['OMPI_', 'PMI_']:\n",
        "            if k.startswith(prefix):\n",
        "                removed_environment[k] = v\n",
        "                del os.environ[k]\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        os.environ.update(removed_environment)\n",
        "\n",
        "\n",
        "class VecFrameStack(VecEnvWrapper):\n",
        "    def __init__(self, venv, nstack):\n",
        "        self.venv = venv\n",
        "        self.nstack = nstack\n",
        "        wos = venv.observation_space  # wrapped ob space\n",
        "        low = np.repeat(wos.low, self.nstack, axis=-1)\n",
        "        high = np.repeat(wos.high, self.nstack, axis=-1)\n",
        "        self.stackedobs = np.zeros((venv.num_envs,) + low.shape, low.dtype)\n",
        "        observation_space = spaces.Box(low=low, high=high, dtype=venv.observation_space.dtype)\n",
        "        VecEnvWrapper.__init__(self, venv, observation_space=observation_space)\n",
        "\n",
        "    def step_wait(self):\n",
        "        obs, rews, news, infos = self.venv.step_wait()\n",
        "        self.stackedobs = np.roll(self.stackedobs, shift=-1, axis=-1)\n",
        "        for (i, new) in enumerate(news):\n",
        "            if new:\n",
        "                self.stackedobs[i] = 0\n",
        "        self.stackedobs[..., -obs.shape[-1]:] = obs\n",
        "        return self.stackedobs, rews, news, infos\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.venv.reset()\n",
        "        self.stackedobs[...] = 0\n",
        "        self.stackedobs[..., -obs.shape[-1]:] = obs\n",
        "        return self.stackedobs\n",
        "\n",
        "class VecExtractDictObs(VecEnvObservationWrapper):\n",
        "    def __init__(self, venv, key):\n",
        "        self.key = key\n",
        "        super().__init__(venv=venv,\n",
        "            observation_space=venv.observation_space.spaces[self.key])\n",
        "\n",
        "    def process(self, obs):\n",
        "        return obs[self.key]\n",
        "\n",
        "\n",
        "class RunningMeanStd(object):\n",
        "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, 'float64')\n",
        "        self.var = np.ones(shape, 'float64')\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
        "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
        "\n",
        "\n",
        "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
        "    delta = batch_mean - mean\n",
        "    tot_count = count + batch_count\n",
        "\n",
        "    new_mean = mean + delta * batch_count / tot_count\n",
        "    m_a = var * count\n",
        "    m_b = batch_var * batch_count\n",
        "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
        "    new_var = M2 / tot_count\n",
        "    new_count = tot_count\n",
        "\n",
        "    return new_mean, new_var, new_count\n",
        "\n",
        "\n",
        "class VecNormalize(VecEnvWrapper):\n",
        "    \"\"\"\n",
        "    A vectorized wrapper that normalizes the observations\n",
        "    and returns from an environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, venv, ob=True, ret=True, clipob=10., cliprew=10., gamma=0.99, epsilon=1e-8):\n",
        "        VecEnvWrapper.__init__(self, venv)\n",
        "\n",
        "        self.ob_rms = RunningMeanStd(shape=self.observation_space.shape) if ob else None\n",
        "        self.ret_rms = RunningMeanStd(shape=()) if ret else None\n",
        "\n",
        "        self.clipob = clipob\n",
        "        self.cliprew = cliprew\n",
        "        self.ret = np.zeros(self.num_envs)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step_wait(self):\n",
        "        obs, rews, news, infos = self.venv.step_wait()\n",
        "        self.ret = self.ret * self.gamma + rews\n",
        "        obs = self._obfilt(obs)\n",
        "        if self.ret_rms:\n",
        "            self.ret_rms.update(self.ret)\n",
        "            rews = np.clip(rews / np.sqrt(self.ret_rms.var + self.epsilon), -self.cliprew, self.cliprew)\n",
        "        self.ret[news] = 0.\n",
        "        return obs, rews, news, infos\n",
        "\n",
        "    def _obfilt(self, obs):\n",
        "        if self.ob_rms:\n",
        "            self.ob_rms.update(obs)\n",
        "            obs = np.clip((obs - self.ob_rms.mean) / np.sqrt(self.ob_rms.var + self.epsilon), -self.clipob, self.clipob)\n",
        "            return obs\n",
        "        else:\n",
        "            return obs\n",
        "\n",
        "    def reset(self):\n",
        "        self.ret = np.zeros(self.num_envs)\n",
        "        obs = self.venv.reset()\n",
        "        return self._obfilt(obs)\n",
        "\n",
        "\n",
        "class VecMonitor(VecEnvWrapper):\n",
        "    def __init__(self, venv, filename=None, keep_buf=0, info_keywords=()):\n",
        "        VecEnvWrapper.__init__(self, venv)\n",
        "        self.eprets = None\n",
        "        self.eplens = None\n",
        "        self.epcount = 0\n",
        "        self.tstart = time.time()\n",
        "\n",
        "        self.results_writer = None\n",
        "        self.info_keywords = info_keywords\n",
        "        self.keep_buf = keep_buf\n",
        "        if self.keep_buf:\n",
        "            self.epret_buf = deque([], maxlen=keep_buf)\n",
        "            self.eplen_buf = deque([], maxlen=keep_buf)\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.venv.reset()\n",
        "        self.eprets = np.zeros(self.num_envs, 'f')\n",
        "        self.eplens = np.zeros(self.num_envs, 'i')\n",
        "        return obs\n",
        "\n",
        "    def step_wait(self):\n",
        "        obs, rews, dones, infos = self.venv.step_wait()\n",
        "        self.eprets += rews\n",
        "        self.eplens += 1\n",
        "\n",
        "        newinfos = list(infos[:])\n",
        "        for i in range(len(dones)):\n",
        "            if dones[i]:\n",
        "                info = infos[i].copy()\n",
        "                ret = self.eprets[i]\n",
        "                eplen = self.eplens[i]\n",
        "                epinfo = {'r': ret, 'l': eplen, 't': round(time.time() - self.tstart, 6)}\n",
        "                for k in self.info_keywords:\n",
        "                    epinfo[k] = info[k]\n",
        "                info['episode'] = epinfo\n",
        "                if self.keep_buf:\n",
        "                    self.epret_buf.append(ret)\n",
        "                    self.eplen_buf.append(eplen)\n",
        "                self.epcount += 1\n",
        "                self.eprets[i] = 0\n",
        "                self.eplens[i] = 0\n",
        "                if self.results_writer:\n",
        "                    self.results_writer.write_row(epinfo)\n",
        "                newinfos[i] = info\n",
        "        return obs, rews, dones, newinfos\n",
        "\n",
        "\n",
        "def moving_average(a, n=3) :\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Helpers \"\"\"\n",
        "\n",
        "def get_avg_score(epinfos):\n",
        "    return np.array([e['r'] for e in epinfos]).mean()\n",
        "\n",
        "# Hiding in here bc it's ugly. Surely better way to get from vecenvs shapes to desired experience replay shape.\n",
        "# Quarantine fine for now.\n",
        "def reshaping_processing_acrobatics(returns_, old_state_estimates_, old_action_probs_, actions_, frames_):\n",
        "\n",
        "    returns_ = returns_.transpose(1,0);\n",
        "    frames_ = frames_.transpose(1,0,2,3,4);\n",
        "    actions_ = actions_.transpose(1,0)\n",
        "    old_action_probs_ = np.array(old_action_probs_).transpose(1,0,2)\n",
        "    old_state_estimates_ = old_state_estimates_.transpose(1,0,2)\n",
        "\n",
        "    actions_ = actions_[..., None].reshape(-1)\n",
        "    returns_ = returns_[..., None].reshape(-1)\n",
        "    frames_ = frames_.reshape(-1, *frames_[0][0].shape);\n",
        "    old_action_probs_ = old_action_probs_[..., None].reshape(-1, n_actions)\n",
        "    old_state_estimates_ = old_state_estimates_.reshape(-1)\n",
        "\n",
        "    returns_ = torch.FloatTensor(returns_).to('cpu')\n",
        "    frames_ = np_to_pytorch_img(frames_) # this op takes forever, should do differently\n",
        "    actions_ = torch.LongTensor(actions_).to('cpu')\n",
        "    old_action_probs_ = torch.FloatTensor(old_action_probs_).to('cpu')\n",
        "    old_state_estimates_ = torch.FloatTensor(old_state_estimates_).to('cpu')\n",
        "\n",
        "    return returns_, old_state_estimates_, old_action_probs_, actions_, frames_\n",
        "\n",
        "def np_to_pytorch_img(frame):\n",
        "    frame = torch.FloatTensor(frame).permute(0,3,1,2) / 255.\n",
        "    return frame\n",
        "\n",
        "def pytorch_to_np_img(frame):\n",
        "    return (frame * 255.).permute(0, 2, 3, 1).numpy().astype(int)\n",
        "\n",
        "categorical = torch.distributions.categorical.Categorical\n",
        "\n",
        "def get_action(action_probs, deterministic=False):\n",
        "    p = action_probs.exp()\n",
        "    if deterministic:\n",
        "        m, ix = torch.max(p, dim=-1);\n",
        "    else:\n",
        "        ix = categorical(p).sample()\n",
        "    return ix\n",
        "\n",
        "def get_n_params(model):\n",
        "    return str(np.round(np.array([p.numel() for p in model.parameters()]).sum() / 1e6, 3)) + ' M params'\n",
        "\n",
        "def resize_observations(observations, new_shape=(160, 210)):\n",
        "    if is_dynamic_heads_model:\n",
        "        new_shape=(158, 206)\n",
        "    else:\n",
        "      new_shape=(84, 84)\n",
        "    resized_observations = []\n",
        "    for ob in observations:\n",
        "\n",
        "        resized = cv2.resize(ob.squeeze(), new_shape, interpolation=cv2.INTER_AREA)\n",
        "        try :\n",
        "          resized = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY)\n",
        "        except:\n",
        "          pass\n",
        "        # Add the channel dimension back\n",
        "        resized = resized[..., np.newaxis]\n",
        "        #print(f\"obs shape: {ob.shape}\")\n",
        "        #if len(ob.shape) == 3:\n",
        "            #resized = resized[..., np.newaxis]  # Add the channel dimension back if it was originally there\n",
        "        resized_observations.append(resized)\n",
        "    return np.array(resized_observations)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aahCauHyAuUj"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj9hCf6LAu7z",
        "outputId": "552726b3-2593-49b1-af44-ffde2989b728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n",
            "env\n",
            "--------------------\n",
            "venv.observation_space: Box(0, 255, (210, 160, 3), uint8)\n",
            "venv.observation_space type: <class 'gymnasium.spaces.box.Box'>\n",
            "venv.observation_space BOX shape: (210, 160, 3)\n",
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n",
            "--------------------\n",
            "<class '__main__.VecMonitor'>\n",
            "--------------------\n",
            "<class '__main__.VecNormalize'>\n",
            "--------------------\n",
            "env.action_space: Discrete(4)\n",
            "env.action_space TYPE: <class 'gymnasium.spaces.discrete.Discrete'>\n",
            "env.action_space.sample(): 3\n",
            "env.observation_space.shape: (210, 160, 3)\n",
            "SB3 Elapsed time: 28.369298219680786 seconds\n",
            "observations.shape: (72, 210, 160, 3)\n",
            "Resized observations shape: (72, 84, 84, 1)\n",
            "Number of actions: 4\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import GrayScaleObservation\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from gymnasium.experimental.wrappers.vector import ResizeObservationV0\n",
        "from gymnasium.wrappers import ResizeObservation\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "env = make_vec_env(env_name, n_envs=n_envs, seed=0)\n",
        "print(type(env))\n",
        "print(f\"env\")\n",
        "print(\"--------------------\")\n",
        "print(f\"venv.observation_space: {env.observation_space}\")\n",
        "print(f\"venv.observation_space type: {type(env.observation_space)}\")\n",
        "print(f\"venv.observation_space BOX shape: {env.observation_space.shape}\")\n",
        "print(type(env))\n",
        "print(\"--------------------\")\n",
        "env = VecMonitor(env)\n",
        "print(type(env))\n",
        "print(\"--------------------\")\n",
        "env = VecNormalize(env, ob=False)\n",
        "print(type(env))\n",
        "print(\"--------------------\")\n",
        "\n",
        "#print(f\"reset:  {env.reset(seed=42)}\")\n",
        "\n",
        "env.reset()\n",
        "\n",
        "print(f\"env.action_space: {env.action_space}\")\n",
        "print(f\"env.action_space TYPE: {type(env.action_space)}\")\n",
        "print(f\"env.action_space.sample(): {env.action_space.sample()}\")\n",
        "print(f\"env.observation_space.shape: {env.observation_space.shape}\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"SB3 Elapsed time: {elapsed_time} seconds\")\n",
        "observations, rewards, dones, infos = env.step([env.action_space.sample() for _ in range(n_envs)])\n",
        "\n",
        "print(f\"observations.shape: {observations.shape}\")\n",
        "#print(observations)\n",
        "\n",
        "resized_obs = resize_observations(observations, new_shape=(64, 64))\n",
        "#resizing due to out of memory issues\n",
        "print(f\"Resized observations shape: {resized_obs.shape}\")\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "print(f\"Number of actions: {n_actions}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VYpI_KXogRJu"
      },
      "source": [
        "## Attention Entropy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IICadKdY5S66"
      },
      "source": [
        "Calculate the mean entropy for each head from the entropy dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMePL_Va6HOm"
      },
      "outputs": [],
      "source": [
        "def calculate_mean_entropies(entropy_dict):\n",
        "    mean_entropies = {}\n",
        "    for head, entropies in entropy_dict.items():\n",
        "        mean_entropies[head] = np.mean(entropies) if entropies else 0.0\n",
        "    return mean_entropies"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RNgU-feL5Ggz"
      },
      "source": [
        "Calculate the entropy of attention weights and return a single entropy value representing the overall entropy across 72 workers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ir92Sn1gX2U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_entropy(attention_weights):\n",
        "    #print(attention_weights.shape)\n",
        "    #Flatten the last dimension since it is always 1\n",
        "    attention_weights = attention_weights.reshape(attention_weights.shape[:-1])\n",
        "    epsilon = 1e-12\n",
        "    attention_weights = np.clip(attention_weights, epsilon, 1. - epsilon)\n",
        "    #Normalise attention weights\n",
        "    attention_weights /= attention_weights.sum(axis=-1, keepdims=True)\n",
        "    #Calculate entropy for each worker\n",
        "    entropy = -np.sum(attention_weights * np.log(attention_weights), axis=-1)\n",
        "    #Calculate average entropy across all workers and time steps\n",
        "    average_entropy = np.mean(entropy)\n",
        "    return average_entropy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3BWIGmrHon-D"
      },
      "source": [
        "## Entropy Coefficient Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTputk0nop8e"
      },
      "outputs": [],
      "source": [
        "def adjust_entropy_coef(entropy_coef, episode, num_episodes):\n",
        "\n",
        "      #print(entropy_coef_end)\n",
        "\n",
        "      #entropy_coef = entropy_coef_end + (entropy_coef_start - entropy_coef_end) * np.log(episode + 1) / np.log(num_episodes)\n",
        "      #print(f\"got here\")\n",
        "\n",
        "      if entropy_decay_type == 'logarithmic':\n",
        "          #Logarithmic decay\n",
        "          #entropy_coef decreases slowly at first and then more rapidly\n",
        "          entropy_coef = entropy_coef_end + (entropy_coef_start - entropy_coef_end) * (entropy_decay ** episode)\n",
        "          #print(f\"logarithmic\")\n",
        "      elif entropy_decay_type == 'exponential':\n",
        "          #Exponential decay\n",
        "          #entropy_coef decreases rapidly at first and then more slowly\n",
        "          entropy_coef = entropy_coef_end + (entropy_coef_start - entropy_coef_end) * np.log(episode + 1) / np.log(num_episodes)\n",
        "          print(f\"entropy_coef {entropy_coef}\")\n",
        "          #print(f\"exponential\")\n",
        "      elif entropy_decay_type == 'stepwise':\n",
        "          #Stepwise decay\n",
        "          #entropy_coef decreases in steps based on the step_decay_interval\n",
        "          decay_steps = episode // step_decay_interval\n",
        "          #print(f\"stepwise\")\n",
        "          entropy_coef = max(entropy_coef_start * (entropy_decay ** decay_steps), entropy_coef_end)\n",
        "\n",
        "      return entropy_coef"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JqB9ki5siInV"
      },
      "source": [
        "## Attention Map Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QwxL5zvt9MD"
      },
      "outputs": [],
      "source": [
        "from matplotlib import animation\n",
        "import shutil\n",
        "import imageio\n",
        "\n",
        "def create_video_from_frames(directory_path, output_video_path, fps=8,):\n",
        "    #Get list of all files in dir\n",
        "    files = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n",
        "    files.sort()\n",
        "\n",
        "    frame = cv2.imread(os.path.join(directory_path, files[0]))\n",
        "    height, width, layers = frame.shape\n",
        "    video = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    for file in files:\n",
        "        frame = cv2.imread(os.path.join(directory_path, file))\n",
        "        video.write(frame)\n",
        "\n",
        "    video.release()\n",
        "    print(f\"Video saved at {output_video_path}\")\n",
        "\n",
        "\n",
        "def generate_attention_maps(env_name, num_frames=100, save_dir=\"./\", first_frame=100, resolution=75, density=5, radius=5, prefix='default', current_frame=None):\n",
        "    print(\"\\n Generating Attention Maps...\\n\")\n",
        "\n",
        "    for head_index in range(4):\n",
        "        head_dir = os.path.join(save_dir, f\"head_{head_index+1}\")\n",
        "        if os.path.exists(head_dir):\n",
        "            shutil.rmtree(head_dir)\n",
        "\n",
        "    meta = get_env_meta(env_name)\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    start = time.time()\n",
        "    movie_title = \"{}-{}-{}.mp4\".format(prefix, num_frames, env_name.lower())\n",
        "    FFMpegWriter = animation.writers['ffmpeg']\n",
        "    metadata = dict(title=movie_title, artist='Daniel Maguire', comment='atari-attention-video')\n",
        "\n",
        "    total_frames = len(frames)\n",
        "    if print_saliency_maps:\n",
        "      print(f\"total frames: {total_frames}\")\n",
        "\n",
        "\n",
        "    for i in range(num_frames): #this skips every second frame so we see a bit more happening in the video\n",
        "      if i % 4 == 0:  #Process only frames that are not skipped\n",
        "        ix = first_frame + i\n",
        "        if ix < total_frames:\n",
        "            if print_saliency_maps:\n",
        "              print(f\"\\n\\n\\n\\n\\nFrame {ix}\")\n",
        "              print(f\"frames shape: {frames.shape}\")\n",
        "            frame = frames[ix].squeeze().numpy().copy()\n",
        "            if print_saliency_maps:\n",
        "              print(f\"frame shape: {frame.shape}\")\n",
        "            #frame = preprocess_frame_saliency_display(frame)\n",
        "            attention_saliencies = score_frame_multiple_heads(agent, None, ix, radius, density, interp_func=occlude, mode='actor')\n",
        "\n",
        "            for head_index, attention_saliency in enumerate(attention_saliencies):\n",
        "                head_dir = os.path.join(save_dir, f\"head_{head_index+1}\")\n",
        "                os.makedirs(head_dir, exist_ok=True)\n",
        "                if print_saliency_maps:\n",
        "                  print(f\"Attention Head {head_index+1}\")\n",
        "\n",
        "                attention_frame, S = saliency_on_atari_frame_multiple_heads([attention_saliency], frame, fudge_factor=meta['actor_ff'], channels=[1], name=f\"Attention Saliency, Head {head_index+1}\")\n",
        "\n",
        "                #Display original Atari frame with overlaid saliency map\n",
        "                if print_saliency_maps:\n",
        "                  plt.figure(figsize=(10, 10))\n",
        "                  plt.imshow(attention_frame, cmap='gray')\n",
        "                  plt.imshow(S, cmap='hot', alpha=0.4)  #Overlay saliency map with alpha blending\n",
        "                  plt.title(f\"Saliency map head {head_index+1}\")\n",
        "                  #plt.savefig(logs_dir_path+\"/saliency_video_frames\")\n",
        "                  frame_filename = os.path.join(head_dir, f\"frame_{ix:04d}.png\")\n",
        "                  plt.savefig(frame_filename)\n",
        "                  plt.show()\n",
        "                  plt.close()\n",
        "                else:\n",
        "                  plt.figure(figsize=(10, 10))\n",
        "                  plt.imshow(attention_frame, cmap='gray')\n",
        "                  plt.imshow(S, cmap='hot', alpha=0.4)  #Overlay saliency map with alpha blending\n",
        "                  plt.title(f\"Saliency map head {head_index+1}\")\n",
        "                  #plt.savefig(logs_dir_path+\"/saliency_video_frames\")\n",
        "                  frame_filename = os.path.join(head_dir, f\"frame_{ix:04d}.png\")\n",
        "                  plt.savefig(frame_filename)\n",
        "                  plt.close()\n",
        "                  plt.close()\n",
        "                  plt.close()\n",
        "                  plt.close()\n",
        "                  plt.close()\n",
        "                  plt.close()\n",
        "                  #Close the plot to free memory\n",
        "\n",
        "            #plt.imshow(frame)\n",
        "            #plt.title(env_name.lower(), fontsize=15)\n",
        "\n",
        "            tstr = time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - start))\n",
        "            print('\\ttime: {} | progress: {:.1f}%'.format(tstr, 100*i/min(num_frames, total_frames)), end='\\r')\n",
        "\n",
        "    directories = ['head_1', 'head_2', 'head_3', 'head_4']\n",
        "    output_videos = ['attention_head_1.mp4', 'attention_head_2.mp4', 'attention_head_3.mp4', 'attention_head_4.mp4']\n",
        "\n",
        "    if current_frame is not None:\n",
        "        attention_vid_dir = os.path.join(save_dir, f\"frame_{current_frame}\")\n",
        "        os.makedirs(attention_vid_dir, exist_ok=True)\n",
        "        for directory, output_video in zip(directories, output_videos):\n",
        "            create_video_from_frames(save_dir+\"/\"+directory, attention_vid_dir+\"/\"+output_video)\n",
        "    else:\n",
        "        for directory, output_video in zip(directories, output_videos):\n",
        "            create_video_from_frames(save_dir+\"/\"+directory, save_dir+\"/\"+output_video)\n",
        "    print('\\nfinished.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKXQrTrliMGU"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage.transform import resize\n",
        "\n",
        "## Attention Map Visualisation Logic\n",
        "\n",
        "def get_env_meta(env_name):\n",
        "    meta = {}\n",
        "    if env_name == \"Breakout-v0\" or env_name == \"Breakout-v4\" or env_name == \"BreakoutNoFrameskip-v4\" or env_name == \"Breakout-v5\":\n",
        "        meta['critic_ff'] = 600\n",
        "        meta['actor_ff'] = 300\n",
        "    elif env_name == \"SpaceInvaders-v0\" or env_name == \"SpaceInvaders-v4\"  or env_name == \"SpaceInvadersNoFrameskip-v4\" or env_name == \"SpaceInvaders-v5\":\n",
        "        meta['critic_ff'] = 400\n",
        "        meta['actor_ff'] = 400\n",
        "    elif env_name == \"Seaquest-v0\" or env_name == \"Seaquest-v4\" or env_name == \"SeaquestNoFrameskip-v4\" or env_name == \"Seaquest-v5\":\n",
        "        meta['critic_ff'] = 400\n",
        "        meta['actor_ff'] = 400\n",
        "    else:\n",
        "        print(f'environment \"{env_name}\" not supported')\n",
        "    return meta\n",
        "\n",
        "\n",
        "searchlight = lambda I, mask: I * mask + gaussian_filter(I, sigma=3) * (1 - mask)  # choose an area NOT to blur\n",
        "occlude = lambda I, mask: I * (1 - mask) + gaussian_filter(I, sigma=3) * mask  # choose an area to blur\n",
        "\n",
        "def plot_attention_weights(history, attention_head_index, num_steps=100):\n",
        "    attention_weights = np.array(history[f'attention_head_{attention_head_index}'])\n",
        "    steps = min(num_steps, len(attention_weights))\n",
        "\n",
        "    fig, axs = plt.subplots(1, steps, figsize=(steps * 2, 2))\n",
        "    for step in range(steps):\n",
        "        ax = axs[step]\n",
        "        ax.imshow(attention_weights[step][0], cmap='hot', interpolation='nearest')\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f'Step {step+1}')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_mask(mask, save_path):\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "def get_mask(center, size, r):\n",
        "    y, x = np.ogrid[-center[0]:size[0] - center[0], -center[1]:size[1] - center[1]]\n",
        "    keep = x * x + y * y <= r * r\n",
        "    mask = np.zeros(size)\n",
        "    mask[keep] = 1  # select a circle of pixels\n",
        "    mask = gaussian_filter(mask, sigma=r)  # blur the circle of pixels. this is a 2D Gaussian for r=r^2=1\n",
        "    return mask / mask.max()\n",
        "\n",
        "\n",
        "def saliency_on_atari_frame_multiple_heads(saliencies, atari, fudge_factor, channels, sigma=0, name=\"\"):\n",
        "    #saliencie: List of saliency maps for different heads\n",
        "    #channels: List of channels to apply each saliency map\n",
        "    pmax = max(saliency.max() for saliency in saliencies)\n",
        "    #print(f\"atari shape: {atari.shape}\")\n",
        "    if print_saliency_maps:\n",
        "      print(f\"atari shape: {atari.shape}\")\n",
        "\n",
        "    if atari.shape[0] == 3:\n",
        "        atari = np.transpose(atari, (1, 2, 0))  #transpose if the channel is the first dimension\n",
        "\n",
        "    #Create a figure for all the subplots\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    #Plot the original Atari frame\n",
        "    #plt.subplot(1, 4, 1)\n",
        "    #plt.title('Original Atari Frame')\n",
        "    #plt.imshow(atari)\n",
        "\n",
        "    if atari.ndim == 2 or (atari.ndim == 3 and atari.shape[2] != 3):\n",
        "        atari = np.expand_dims(atari, axis=-1)\n",
        "\n",
        "    I = atari.astype(np.float32)\n",
        "\n",
        "\n",
        "    for saliency, channel in zip(saliencies, channels):\n",
        "        #Resize saliency to the size of Atari frame\n",
        "        S = resize(saliency, (atari.shape[0], atari.shape[1]), mode='reflect', preserve_range=True).astype(np.float32)\n",
        "\n",
        "        if sigma > 0:\n",
        "            S = gaussian_filter(S, sigma=sigma)  #Apply Gaussian blur if sigma > 0\n",
        "\n",
        "        S -= S.min()  #Normalize saliency map\n",
        "        S = fudge_factor * pmax * S / S.max()  #Scale saliency map\n",
        "\n",
        "        S = np.clip(S, 0, 255)  #Clip values to the range [0, 255]\n",
        "\n",
        "        if I.shape[2] == 1:\n",
        "            # If the image has a single channel, we simply add the saliency map to it\n",
        "            I[:, :, 0] = np.clip(I[:, :, 0] + S, 0, 255)\n",
        "        else:\n",
        "            # Otherwise, add to the specified channel\n",
        "            I[:, :, channel] = np.clip(I[:, :, channel] + S, 0, 255)\n",
        "\n",
        "    I = I.astype(np.uint8)  #Convert image back to uint8\n",
        "    '''\n",
        "        #Display the original Atari frame with overlaid saliency map\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(atari)\n",
        "        plt.imshow(S, cmap='hot', alpha=0.4)  #Overlay saliency map with alpha blending\n",
        "        plt.title(name)\n",
        "        plt.show()\n",
        "\n",
        "        #plt.savefig(logs_dir_path+\"/saliency_video_frames\")\n",
        "        plt.close()  # Close the plot to free memory\n",
        "    '''\n",
        "    return atari, S\n",
        "\n",
        "def score_frame_multiple_heads(model, history, ix, r, d, interp_func, mode='actor'):\n",
        "    assert mode in ['actor', 'critic'], 'mode must be either \"actor\" or \"critic\"'\n",
        "\n",
        "    #Combining all attention heads into a single list for iteration\n",
        "    attention_heads = [\n",
        "        attention_head_1,\n",
        "        attention_head_2,\n",
        "        attention_head_3,\n",
        "        attention_head_4\n",
        "    ]\n",
        "    #print(f\"Number of attention heads: {len(attention_heads)}\")\n",
        "\n",
        "    saliencies = []\n",
        "    for head_index in range(len(attention_heads)):\n",
        "        L = run_through_attention_model(model, history, ix, interp_func, mask=None, mode=mode, head_index=head_index)\n",
        "        scores = np.zeros((int(84 / d) + 1, int(84 / d) + 1))  # saliency scores S(t,i,j)\n",
        "        for i in range(0, 84, d):\n",
        "            for j in range(0, 84, d):\n",
        "                mask = get_mask(center=[i, j], size=[84, 84], r=r)\n",
        "                l = run_through_attention_model(model, history, ix, interp_func, mask=mask, mode=mode, head_index=head_index)\n",
        "                scores[int(i / d), int(j / d)] = (L - l).pow(2).sum().mul_(.5).item()\n",
        "\n",
        "        pmax = scores.max()\n",
        "        scores = resize(scores, (84, 84), mode='reflect', preserve_range=True).astype(np.float32)\n",
        "        saliencies.append(pmax * scores / scores.max())\n",
        "\n",
        "    return saliencies\n",
        "\n",
        "def run_through_attention_model(model, history, ix, interp_func=None, mask=None, blur_memory=None, mode='actor', head_index=0):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if mask is None:\n",
        "        #im = preprocess_observation(history['ins'][ix])\n",
        "        im = frames[ix]\n",
        "        #print(f\"im shape: {im.shape}\")\n",
        "    else:\n",
        "        assert interp_func is not None, \"interp func cannot be none\"\n",
        "        #im = interp_func(preprocess_observation(history['ins'][ix]).squeeze(), mask).reshape(1, 80, 80)  # perturb input I -> I'\n",
        "\n",
        "        im = interp_func(frames[ix].squeeze(), mask).reshape(1, 84, 84)  # perturb input I -> I'\n",
        "        #print(f\"im shape: {im.shape}\")\n",
        "\n",
        "    tens_state = im.clone().detach().to(device).float()  # Convert to float\n",
        "\n",
        "    state = Variable(tens_state.unsqueeze(0)).to(device)\n",
        "    hx = Variable(torch.tensor(history_hx[ix-1], dtype=torch.float32).view(1, -1)).to(device)\n",
        "    cx = Variable(torch.tensor(history_cx[ix-1], dtype=torch.float32).view(1, -1)).to(device)\n",
        "\n",
        "    if blur_memory is not None:\n",
        "        cx.mul_(1 - blur_memory)  # perturb memory vector\n",
        "\n",
        "    state_values, action_score_logits, hidden_state, attention_maps = model((state))\n",
        "    return attention_maps[head_index]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CoG0TYIY8y0X"
      },
      "source": [
        "# Model Implementations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "64WMxhSXnxNA"
      },
      "source": [
        "## Attention IMPALA Logic"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dnqy2vG55hw1"
      },
      "source": [
        "### ConvLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ghc2ZCA5jrY",
        "outputId": "3e33c1c4-ea45-43a9-e703-2dfa37cd6d60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
        "        \"\"\"Initialize stateful ConvLSTM cell.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_channels : ``int``\n",
        "            Number of channels of input tensor.\n",
        "        hidden_channels : ``int``\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size : ``int``\n",
        "            Size of the convolutional kernel.\n",
        "\n",
        "        Paper\n",
        "        -----\n",
        "        https://papers.nips.cc/paper/5955-convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting.pdf\n",
        "\n",
        "        Referenced code\n",
        "        ---------------\n",
        "        https://github.com/automan000/Convolution_LSTM_PyTorch/blob/master/convolution_lstm.py\n",
        "        \"\"\"\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        assert hidden_channels % 2 == 0\n",
        "\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_features = 4\n",
        "\n",
        "        self.padding = int((kernel_size - 1) / 2)\n",
        "\n",
        "        self.Wxi = nn.Conv2d(\n",
        "            self.input_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.Whi = nn.Conv2d(\n",
        "            self.hidden_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.Wxf = nn.Conv2d(\n",
        "            self.input_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.Whf = nn.Conv2d(\n",
        "            self.hidden_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.Wxc = nn.Conv2d(\n",
        "            self.input_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.Whc = nn.Conv2d(\n",
        "            self.hidden_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.Wxo = nn.Conv2d(\n",
        "            self.input_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.Who = nn.Conv2d(\n",
        "            self.hidden_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=False,\n",
        "        )\n",
        "\n",
        "        self.Wci = None\n",
        "        self.Wcf = None\n",
        "        self.Wco = None\n",
        "\n",
        "        self.prev_hidden = None\n",
        "\n",
        "    def register_hooks(self, tensor, name):\n",
        "        def hook(grad):\n",
        "            print(f\"Gradient for {name}: {grad}\")\n",
        "        tensor.register_hook(hook)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if print_model_logs : print(f\"[ConvLSTMCell] Input x shape: {x.shape}\")\n",
        "        #if the batch size changes then reset prev_hidden\n",
        "        if self.prev_hidden is None or self.prev_hidden[0].shape[0] != x.size()[0]:\n",
        "            #Initialise hidden and cell states if not previously provided or if wrong batch size shape\n",
        "            if print_model_logs : print(f\"[ConvLSTMCell] Initializing hidden state.\")\n",
        "            batch_size, _, height, width = x.size()\n",
        "            h, c = self.init_hidden(\n",
        "                batch_size, self.hidden_channels, height, width, x.device\n",
        "            )\n",
        "        else:\n",
        "            h, c = self.prev_hidden\n",
        "            h = h.detach()\n",
        "            c = c.detach()\n",
        "\n",
        "        #LSTM gate operations with convolution\n",
        "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
        "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
        "        cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
        "        co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
        "        ch = co * torch.tanh(cc)\n",
        "        #with torch.no_grad():\n",
        "        #ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + (c * self.Wci if self.Wci is not None else 0))\n",
        "        #cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + (c * self.Wcf if self.Wcf is not None else 0))\n",
        "\n",
        "        #cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
        "\n",
        "        #with torch.no_grad():\n",
        "        #co = torch.sigmoid(self.Wxo(x) + self.Who(h) + (cc * self.Wco if self.Wco is not None else 0))\n",
        "        #print(f\"[ConvLSTMCell] ci: {ci.shape}, cf: {cf.shape}, cc: {cc.shape}, co: {co.shape}\")\n",
        "\n",
        "\n",
        "        #with torch.no_grad():\n",
        "        #tancc = torch.tanh(cc)\n",
        "          #newco = co\n",
        "\n",
        "        #print(f\"[ConvLSTMCell] ci: {ci.shape}, cf: {cf.shape}, cc: {cc.shape}, co: {co.shape} tancc: {tancc.shape}\")\n",
        "        #ch = co * tancc\n",
        "\n",
        "        if print_logs :\n",
        "          print(f\"[ConvLSTMCell] Output ch shape: {ch.shape}, cc shape: {cc.shape}\")\n",
        "        #print(f\"ci: {ci}, cf: {cf}, cc: {cc}, co: {co}, ch: {ch}\")\n",
        "\n",
        "        #if ch.grad is not None: print(f\"ch.grad: {ch.grad.shape}\")\n",
        "        #if cc.grad is not None: print(f\"cc.grad: {cc.grad.shape}\")\n",
        "\n",
        "\n",
        "        self.prev_hidden = ch, cc\n",
        "\n",
        "        #self.register_hooks(x, \"x\")\n",
        "        #self.register_hooks(h, \"h\")\n",
        "        #self.register_hooks(c, \"c\")\n",
        "\n",
        "        # Register hooks\n",
        "        #self.register_hooks(ci, \"ci\")\n",
        "        #self.register_hooks(cf, \"cf\")\n",
        "        #self.register_hooks(cc, \"cc\")\n",
        "        #self.register_hooks(co, \"co\")\n",
        "        #self.register_hooks(ch, \"ch\")\n",
        "\n",
        "        if print_logs : print(f\"[ConvLSTMCell] Output shapes: h: {h.shape}, c: {c.shape}\")\n",
        "        return ch, cc\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the cell's hidden state.\"\"\"\n",
        "        self.prev_hidden = None\n",
        "\n",
        "    def init_hidden(self, batch_size, hidden, height, width, device):\n",
        "        if self.Wci is None:\n",
        "            #Initialise cell state weights if they are not initialised already\n",
        "            self.Wci = torch.zeros(1, hidden, height, width, requires_grad=True).to(\n",
        "                device\n",
        "            )\n",
        "            self.Wcf = torch.zeros(1, hidden, height, width, requires_grad=True).to(\n",
        "                device\n",
        "            )\n",
        "            self.Wco = torch.zeros(1, hidden, height, width, requires_grad=True).to(\n",
        "                device\n",
        "            )\n",
        "        return (\n",
        "            torch.zeros(batch_size, hidden, height, width, requires_grad=True).to(\n",
        "                device\n",
        "            ),\n",
        "            torch.zeros(batch_size, hidden, height, width, requires_grad=True).to(\n",
        "                device\n",
        "            ),\n",
        "        )\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x_VrH5TcAmKt"
      },
      "source": [
        "### Query Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGcsRBpcAh2w"
      },
      "outputs": [],
      "source": [
        "class QueryNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "        Network for generating queries for attention\n",
        "\n",
        "        Params:\n",
        "        - c_k (int): Dimension of key vectors\n",
        "        - c_s (int): Dimension of spatial basis vectors\n",
        "        - num_queries (int): Number of attention heads\n",
        "    \"\"\"\n",
        "    def __init__(self, c_k, c_s, num_queries):\n",
        "        super(QueryNetwork, self).__init__()\n",
        "        self.c_k = c_k\n",
        "        self.c_s = c_s\n",
        "        self.num_queries = num_queries\n",
        "\n",
        "        #Feedforward network for query generation\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(256, 128),  #First linear transformation\n",
        "            nn.LayerNorm(128),    #Normalisation for stability\n",
        "            nn.ReLU(),            #Activation for non linearity\n",
        "            nn.Linear(128, 128),  #Second linear transformation\n",
        "            nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1280),  #Last layer outputs 1280 features\n",
        "            nn.LayerNorm(1280),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        #Transformation layer reshapes the output to the expected dimensions\n",
        "        self.output_transform = nn.Linear(1280, (c_k + c_s) * num_queries)\n",
        "\n",
        "    def forward(self, query):\n",
        "        out = self.model(query)\n",
        "        out = self.output_transform(out)  # Transform output to match (c_k + c_s) * num_queries\n",
        "        out = out.reshape(-1, self.num_queries, self.c_k + self.c_s)  # Reshape for query output\n",
        "        if print_logs: print(f\"[QueryNetwork] Output shape: {out.shape}\")\n",
        "        #print(f\"[QueryNetwork] Output shape: {out.shape}\")\n",
        "        return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ErvvA1Gt-0WG"
      },
      "source": [
        "### Spatial Basis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bxzO6hX-uUz"
      },
      "outputs": [],
      "source": [
        "class SpatialBasis:\n",
        "    \"\"\"\n",
        "        Class to generate spatial basis, which augments input features with spatial info\n",
        "\n",
        "        Params:\n",
        "        - channels (int): Number of output channels after applying spatial basis\n",
        "    \"\"\"\n",
        "    def __init__(self, channels=64):\n",
        "        self.channels = channels\n",
        "\n",
        "    def __call__(self, X):\n",
        "        batch_size, h, w, _ = X.size()\n",
        "\n",
        "        #Generating position encodings for height and width\n",
        "        p_h = torch.mul(torch.arange(1, h+1).unsqueeze(1).float(), torch.ones(1, w).float()) * (np.pi / h)\n",
        "        p_w = torch.mul(torch.ones(h, 1).float(), torch.arange(1, w+1).unsqueeze(0).float()) * (np.pi / w)\n",
        "\n",
        "        U = V = 8 #size of U, V\n",
        "        u_basis = v_basis = torch.arange(1, U+1).unsqueeze(0).float() #Size of basis vectors\n",
        "        a = torch.mul(p_h.unsqueeze(2), u_basis) #Positional encodings for U basis\n",
        "        b = torch.mul(p_w.unsqueeze(2), v_basis) #Positional encodings for V basis\n",
        "        out = torch.einsum('hwu,hwv->hwuv', torch.cos(a), torch.cos(b)).reshape(h, w, self.channels).to(X.device)\n",
        "\n",
        "        S = out.unsqueeze(0).expand(batch_size, -1, -1, -1) #Expand across batch\n",
        "        return torch.cat([X, S], dim=3) #Concatenate spatial basis with input tensor\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4HS_fk4O-2y8"
      },
      "source": [
        "### Spatial Softmax"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QJkJAoS47ISA"
      },
      "source": [
        "Applies a softmax over spatial dims (height and width) of the input tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nalV30qO-wXk"
      },
      "outputs": [],
      "source": [
        "def spatial_softmax(A):\n",
        "    #A: batch_size x h x w x d\n",
        "    b, h, w, d = A.size()\n",
        "    #Flatten A s.t. softmax is applied to each grid point, not over each queries\n",
        "    A = A.reshape(b, h * w, d)\n",
        "    #Applies softmax over flattened spatial dimension\n",
        "    A = F.softmax(A, dim=1)\n",
        "    #Reshape it back to original spatial dims\n",
        "    A = A.reshape(b, h, w, d)\n",
        "    return A"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kQPZvnIt-5f0"
      },
      "source": [
        "### Apply Alpha"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jNEUfQVA7Lms"
      },
      "source": [
        "Applies attention weights to the value tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQgxfyV_-siq"
      },
      "outputs": [],
      "source": [
        "def apply_alpha(A, V):\n",
        "    b, h, w, c = A.size()\n",
        "    #Flatten and transpose for matmul\n",
        "    A = A.reshape(b, h * w, c).transpose(1, 2)\n",
        "    _, _, _, d = V.size()\n",
        "    #Flatten value tensor for matmul\n",
        "    V = V.reshape(b, h * w, d)\n",
        "    #Apply attention weights to values\n",
        "    return torch.matmul(A, V)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UEfvbA0dbUDi"
      },
      "source": [
        "### Attention IMPALA Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfGe51UtbW8-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from utils import *\n",
        "\n",
        "class Flatten2(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.contiguous().reshape(x.size(0), -1)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, n_channels):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(n_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.ReLU()(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn1(out)\n",
        "        out = nn.ReLU()(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        return out + x\n",
        "\n",
        "class ImpalaBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ImpalaBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1)\n",
        "        self.res1 = ResBlock(out_channels)\n",
        "        self.res2 = ResBlock(out_channels)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn(x)\n",
        "        x = nn.MaxPool2d(kernel_size=3, stride=2)(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        return x\n",
        "\n",
        "class AttentionImpala(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hidden_size=256,\n",
        "                 num_queries=4,\n",
        "                 c_v: int = 16,\n",
        "                 c_k: int = 16,\n",
        "                 c_s: int = 64,):\n",
        "        super(AttentionImpala, self).__init__()\n",
        "        self.block1 = ImpalaBlock(in_channels=1, out_channels=16)\n",
        "        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n",
        "        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n",
        "        self.fc = nn.Linear(256, 256)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.c_v = c_v\n",
        "        self.c_k = c_k\n",
        "        self.c_s = c_s\n",
        "        self.num_queries = num_queries\n",
        "\n",
        "        self.critic = init_critic_(nn.Linear(256, 1))\n",
        "        self.actor = init_actor_(nn.Linear(256, n_actions))\n",
        "\n",
        "        self.vision_lstm = ConvLSTMCell(\n",
        "            input_channels=32,\n",
        "            hidden_channels=32,\n",
        "            kernel_size=3\n",
        "        )\n",
        "\n",
        "        self.answer_processor = nn.Sequential(\n",
        "            # 1026 x 512\n",
        "            nn.Linear(\n",
        "                (c_v + c_s) * num_queries + (c_k + c_s) * num_queries + 1 + 1, 512\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, hidden_size),\n",
        "        )\n",
        "\n",
        "        self.spatial = SpatialBasis()\n",
        "        self.query = QueryNetwork(c_k, c_s, num_queries)\n",
        "\n",
        "        self.prev_output = None\n",
        "        self.prev_hidden = None\n",
        "        self.policy_core = nn.LSTMCell(hidden_size, hidden_size) # Core LSTM for policy decision-making\n",
        "\n",
        "        #Initialises attention head mask\n",
        "        self.attention_mask = torch.ones(num_queries, dtype=torch.bool)\n",
        "\n",
        "    def set_attention_mask(self, mask):\n",
        "        #Sets mask for attention heads\n",
        "        self.attention_mask = mask\n",
        "\n",
        "\n",
        "    def forward(self, x, prev_reward=None, prev_action=None):\n",
        "\n",
        "        batch_size = x.size()[0]\n",
        "        self.attention_mask = self.attention_mask.to(x.device)\n",
        "\n",
        "        if prev_reward is None:\n",
        "            # (n, 1, 1)\n",
        "            prev_reward = torch.stack([torch.zeros(1, 1)] * batch_size).to(x.device)\n",
        "        else:\n",
        "            prev_reward = torch.tensor(prev_reward).clone().detach().to(device)\n",
        "            prev_reward = prev_reward.unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "        if prev_action is None:\n",
        "            # (n, 1, 1)\n",
        "            prev_action = torch.stack([torch.zeros(1, 1)] * batch_size).to(x.device)\n",
        "        else:\n",
        "            prev_action = torch.tensor(prev_action).clone().detach().to(device)\n",
        "            prev_action = prev_action.unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "        #print(f\"Input shape: {x.shape}\")  # Print input shape\n",
        "        x = self.block1(x)\n",
        "        #print(f\"Shape after block1: {x.shape}\")  # Print shape after block1\n",
        "        x = self.block2(x)\n",
        "        #print(f\"Shape after block2: {x.shape}\")  # Print shape after block2\n",
        "        x = self.block3(x)\n",
        "        #print(f\"Shape after block3: {x.shape}\")  # Print shape after block3\n",
        "        x = nn.ReLU()(x)\n",
        "\n",
        "        x, _ = self.vision_lstm(x)\n",
        "        '''\n",
        "        x = Flatten()(x)\n",
        "        #print(f\"Shape after flatten: {x.shape}\")  # Print shape after flatten\n",
        "        x = self.fc(x)\n",
        "        #print(f\"Shape after fc: {x.shape}\")  # Print shape after fc\n",
        "\n",
        "\n",
        "        x = nn.ReLU()(x)\n",
        "\n",
        "        '''\n",
        "        x = x.transpose(1, 3)\n",
        "\n",
        "        batch_size = x.size()[0]\n",
        "        #print(f\"self.c_k: {self.c_k}\")\n",
        "        #print(f\"self.c_s: {self.c_s}\")\n",
        "        #print(f\"self.c_v: {self.c_v}\")\n",
        "        #print(f\"x: {x.shape}\")\n",
        "        #Splitting the vision modules output to get Keys, and Values\n",
        "        K, V = x.split([self.c_k, self.c_v], dim=3)\n",
        "\n",
        "        #if print_logs:\n",
        "        #print(f\"K SPLIT: {K.shape}\")\n",
        "        #print(f\"V SPLIT: {V.shape}\")\n",
        "        #log_with_timestamp(f\"[Agent] Split K shape: {K.shape}, V shape: {V.shape}\", logs_dir_path+\"/debug.log\")\n",
        "        K, V = self.spatial(K), self.spatial(V)\n",
        "        #log_with_timestamp(f\"[Agent] After Spatial K shape: {K.shape}, V shape: {V.shape}\", logs_dir_path+\"/debug.log\")\n",
        "        #if print_logs:\n",
        "        #print(f\"K SPATIAL: {K.shape}\")\n",
        "        #print(f\"V SPATIAL: {V.shape}\")\n",
        "\n",
        "        # 1 (b). Queries.\n",
        "        # --------------\n",
        "\n",
        "\n",
        "        if self.prev_output is None or self.prev_output.shape[0]!=batch_size:\n",
        "            hidden_state = torch.zeros(\n",
        "                batch_size, self.hidden_size, requires_grad=True\n",
        "            ).to(x.device)\n",
        "            self.prev_output = hidden_state\n",
        "\n",
        "        # (n, h, w, num_queries, c_k + c_s)\n",
        "        Q = self.query(self.prev_output)\n",
        "\n",
        "\n",
        "        #Apply attention mask\n",
        "        Q = Q * self.attention_mask.unsqueeze(0).unsqueeze(2).float()\n",
        "\n",
        "        #print(f\"self.prev_output : {self.prev_output.shape[0]}\")\n",
        "        #print(f\"batch_size : {batch_size}\")\n",
        "\n",
        "        # 2. Answer.\n",
        "        # ----------\n",
        "        # (n, h, w, num_queries)\n",
        "        if print_logs:\n",
        "          print(f\"K shape: {K.shape}\")\n",
        "          print(f\"Q shape: {Q.shape}\")\n",
        "          print(f\"V shape: {V.shape}\")\n",
        "\n",
        "        #transposed_Q = Q.transpose(2, 1).unsqueeze(1)\n",
        "        #print(f\"transposed_Q shape: {transposed_Q.shape}\")\n",
        "\n",
        "        #print(f\"K.shape {K.shape}\")\n",
        "        #print(f\"Q.shape {Q.shape}\")\n",
        "        #print(f\"Q.shape TRANSPOSED  {Q.transpose(2, 1).unsqueeze(1).shape}\")\n",
        "\n",
        "        A = torch.matmul(K, Q.transpose(2, 1).unsqueeze(1))\n",
        "        # (n, h, w, num_queries)\n",
        "        if print_logs:\n",
        "          print(f\"A shape: {A.shape} - (n, h, w, num_queries)\")\n",
        "        A = spatial_softmax(A)\n",
        "        if print_logs:\n",
        "          print(f\"spatial_softmax(A): {A.shape} -(n, 1, 1, num_queries)\")\n",
        "        # (n, 1, 1, num_queries)\n",
        "        a = apply_alpha(A, V)\n",
        "\n",
        "        # (n, (c_v + c_s) * num_queries + (c_k + c_s) * num_queries + 1 + 1)\n",
        "        answer = torch.cat(\n",
        "            torch.chunk(a, 4, dim=1)\n",
        "            + torch.chunk(Q, 4, dim=1)\n",
        "            + (prev_reward.float(), prev_action.float()),\n",
        "            dim=2,\n",
        "        ).squeeze(1)\n",
        "        # (n, hidden_size)\n",
        "\n",
        "        x = self.answer_processor(answer)\n",
        "\n",
        "         # 3. Policy Core LSTM\n",
        "        # ----------\n",
        "        if self.prev_hidden is None:\n",
        "            h, c = self.policy_core(x)\n",
        "        else:\n",
        "            h, c = self.policy_core(x, (self.prev_output, self.prev_hidden))\n",
        "            self.prev_output, self.prev_hidden = h, c\n",
        "        # (n, hidden_size)\n",
        "        x = h\n",
        "\n",
        "        #x = Flatten()(x)\n",
        "        #print(f\"Shape after flatten: {x.shape}\")  # Print shape after flatten\n",
        "        #x = self.fc(x)\n",
        "        #print(f\"Shape after fc: {x.shape}\")  # Print shape after fc\n",
        "\n",
        "        #x = nn.ReLU()(x)\n",
        "\n",
        "        c = self.critic(x) # state value\n",
        "        a = nn.LogSoftmax(dim=-1)(self.actor(x))  # action logits\n",
        "\n",
        "        return a, c, (h, c), A.split(1, dim=-1) # Attention passed out so we can visualise it\n",
        "\n",
        "# Proper orthogonal init in the right locations is important\n",
        "def init(module, weight_init, bias_init, gain=1):\n",
        "    weight_init(module.weight.data, gain=gain)\n",
        "    bias_init(module.bias.data)\n",
        "    return module\n",
        "\n",
        "init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0), nn.init.calculate_gain('relu'))\n",
        "init_critic_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0))\n",
        "init_actor_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.constant_(x, 0), gain=0.01)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O7SQkThZfPaL"
      },
      "source": [
        "## Dynamic Heads Paper Attention Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJR6ss00fXM2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "print_model_logs = False\n",
        "print_logs = False\n",
        "\n",
        "class ConvLSTMCellDynamicHeads(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
        "        \"\"\"Initialize stateful ConvLSTM cell.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_channels : ``int``\n",
        "            Number of channels of input tensor.\n",
        "        hidden_channels : ``int``\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size : ``int``\n",
        "            Size of the convolutional kernel.\n",
        "\n",
        "        Paper\n",
        "        -----\n",
        "        https://papers.nips.cc/paper/5955-convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting.pdf\n",
        "\n",
        "        Referenced code\n",
        "        ---------------\n",
        "        https://github.com/automan000/Convolution_LSTM_PyTorch/blob/master/convolution_lstm.py\n",
        "        \"\"\"\n",
        "        super(ConvLSTMCellDynamicHeads, self).__init__()\n",
        "\n",
        "        assert hidden_channels % 2 == 0\n",
        "\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_features = 4\n",
        "\n",
        "        self.padding = int((kernel_size - 1) / 2)\n",
        "\n",
        "        self.Wxi = nn.Conv2d(\n",
        "            self.input_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.Whi = nn.Conv2d(\n",
        "            self.hidden_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.Wxf = nn.Conv2d(\n",
        "            self.input_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.Whf = nn.Conv2d(\n",
        "            self.hidden_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.Wxc = nn.Conv2d(\n",
        "            self.input_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.Whc = nn.Conv2d(\n",
        "            self.hidden_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.Wxo = nn.Conv2d(\n",
        "            self.input_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=True,\n",
        "        )\n",
        "        self.Who = nn.Conv2d(\n",
        "            self.hidden_channels,\n",
        "            self.hidden_channels,\n",
        "            self.kernel_size,\n",
        "            1,\n",
        "            self.padding,\n",
        "            bias=False,\n",
        "        )\n",
        "\n",
        "        self.Wci = None\n",
        "        self.Wcf = None\n",
        "        self.Wco = None\n",
        "\n",
        "        self.prev_hidden = None\n",
        "\n",
        "    def register_hooks(self, tensor, name):\n",
        "        def hook(grad):\n",
        "            print(f\"Gradient for {name}: {grad}\")\n",
        "        tensor.register_hook(hook)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if print_model_logs : print(f\"[ConvLSTMCell] Input x shape: {x.shape}\")\n",
        "        #if the batch size changes then reset prev_hidden\n",
        "        if self.prev_hidden is None or self.prev_hidden[0].shape[0] != x.size()[0]:\n",
        "            #Initialise hidden and cell states if not previously provided or if wrong batch size shape\n",
        "            if print_model_logs : print(f\"[ConvLSTMCell] Initializing hidden state.\")\n",
        "            batch_size, _, height, width = x.size()\n",
        "            h, c = self.init_hidden(\n",
        "                batch_size, self.hidden_channels, height, width, x.device\n",
        "            )\n",
        "        else:\n",
        "            h, c = self.prev_hidden\n",
        "\n",
        "        #LSTM gate operations with convolution\n",
        "        #ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + c * self.Wci)\n",
        "        #cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + c * self.Wcf)\n",
        "        #cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
        "        #co = torch.sigmoid(self.Wxo(x) + self.Who(h) + cc * self.Wco)\n",
        "        #ch = co * torch.tanh(cc)\n",
        "        #with torch.no_grad():\n",
        "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h) + (c * self.Wci if self.Wci is not None else 0))\n",
        "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h) + (c * self.Wcf if self.Wcf is not None else 0))\n",
        "\n",
        "        cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
        "\n",
        "        with torch.no_grad():\n",
        "          co = torch.sigmoid(self.Wxo(x) + self.Who(h) + (cc * self.Wco if self.Wco is not None else 0))\n",
        "        #print(f\"[ConvLSTMCell] ci: {ci.shape}, cf: {cf.shape}, cc: {cc.shape}, co: {co.shape}\")\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          tancc = torch.tanh(cc)\n",
        "          #newco = co\n",
        "\n",
        "        #print(f\"[ConvLSTMCell] ci: {ci.shape}, cf: {cf.shape}, cc: {cc.shape}, co: {co.shape} tancc: {tancc.shape}\")\n",
        "        ch = co * tancc\n",
        "\n",
        "        if print_logs :\n",
        "          print(f\"[ConvLSTMCell] Output ch shape: {ch.shape}, cc shape: {cc.shape}\")\n",
        "        #print(f\"ci: {ci}, cf: {cf}, cc: {cc}, co: {co}, ch: {ch}\")\n",
        "\n",
        "        #if ch.grad is not None: print(f\"ch.grad: {ch.grad.shape}\")\n",
        "        #if cc.grad is not None: print(f\"cc.grad: {cc.grad.shape}\")\n",
        "\n",
        "\n",
        "        self.prev_hidden = ch, cc\n",
        "\n",
        "\n",
        "        #self.register_hooks(x, \"x\")\n",
        "        #self.register_hooks(h, \"h\")\n",
        "        #self.register_hooks(c, \"c\")\n",
        "\n",
        "        # Register hooks\n",
        "        #self.register_hooks(ci, \"ci\")\n",
        "        #self.register_hooks(cf, \"cf\")\n",
        "        #self.register_hooks(cc, \"cc\")\n",
        "        #self.register_hooks(co, \"co\")\n",
        "        #self.register_hooks(ch, \"ch\")\n",
        "\n",
        "        if print_logs : print(f\"[ConvLSTMCell] Output shapes: h: {h.shape}, c: {c.shape}\")\n",
        "        return ch, cc\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the cell's hidden state.\"\"\"\n",
        "        self.prev_hidden = None\n",
        "\n",
        "    def init_hidden(self, batch_size, hidden, height, width, device):\n",
        "        if self.Wci is None:\n",
        "            #Initialise cell state weights if they are not initialised already\n",
        "            self.Wci = torch.zeros(1, hidden, height, width, requires_grad=True).to(\n",
        "                device\n",
        "            )\n",
        "            self.Wcf = torch.zeros(1, hidden, height, width, requires_grad=True).to(\n",
        "                device\n",
        "            )\n",
        "            self.Wco = torch.zeros(1, hidden, height, width, requires_grad=True).to(\n",
        "                device\n",
        "            )\n",
        "        return (\n",
        "            torch.zeros(batch_size, hidden, height, width, requires_grad=True).to(\n",
        "                device\n",
        "            ),\n",
        "            torch.zeros(batch_size, hidden, height, width, requires_grad=True).to(\n",
        "                device\n",
        "            ),\n",
        "        )\n",
        "\n",
        "class VisionNetworkDynamicHeads(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VisionNetworkDynamicHeads, self).__init__()\n",
        "        #CNN architecture for feature extraction\n",
        "        self.vision_cnn = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        #1x1 Convolution to reduce channels from 256 to 64\n",
        "        self.channel_transform = nn.Sequential(\n",
        "            nn.Conv2d(256, 64, 1),  # 1x1 convolution\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        #rnndown\n",
        "        self.vision_lstm = ConvLSTMCell(\n",
        "            input_channels=64,\n",
        "            hidden_channels=64,\n",
        "            kernel_size=3\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the LSTM state for a new sequence.\"\"\"\n",
        "        self.vision_lstm.reset()\n",
        "\n",
        "    def register_hooks(self, tensor, name):\n",
        "        def hook(grad):\n",
        "            print(f\"Gradient for {name}: {grad}\")\n",
        "        tensor.register_hook(hook)\n",
        "\n",
        "    def forward(self, X):\n",
        "        if print_logs:\n",
        "          print(f\"[VisionNetwork] Input X shape: {X.shape}\")\n",
        "        #if X.shape == (1, 1, 206, 158):\n",
        "            #log_with_timestamp(f\"[VisionNetwork] Input is shape. X shape: {X.shape}, no need to transpose\", logs_dir_path+\"/debug.log\")\n",
        "        if X.shape == (1, 158, 206, 1):\n",
        "            X = X.transpose(1, 3)  # Adjusting the channels to align with PyTorch's format\n",
        "        #else :\n",
        "            #if print_logs:\n",
        "              #print(f\"[VisionNetwork] Input shape is: {X.shape}\")\n",
        "\n",
        "        #print(f\"[VisionNetwork] Input X shape: {X.shape}\")\n",
        "        #X = X.transpose(1, 3)  # Adjusting the channels to align with PyTorch's format\n",
        "        #print(f\"[VisionNetwork] After transpose X shape: {X.shape}\")\n",
        "        with torch.no_grad():\n",
        "          X = self.vision_cnn(X)  # Passing through the CNN\n",
        "        #print(f\"[VisionNetwork] After CNN X shape: {X.shape}\")\n",
        "\n",
        "          X = self.channel_transform(X)  # Transforming the channel dimensions from 256 to 64\n",
        "        #print(f\"[VisionNetwork] After channel transform X shape: {X.shape}\")\n",
        "\n",
        "        O, _ = self.vision_lstm(X)  # Passing the transformed output into ConvLSTM\n",
        "\n",
        "\n",
        "        # Register hooks\n",
        "        #self.register_hooks(O, \"O\")\n",
        "        if print_logs:\n",
        "          print(f\"[VisionNetwork] Output O shape: {O.shape}\")\n",
        "\n",
        "        return O.transpose(1, 3)  # Re-adjusting the output format if needed\n",
        "\n",
        "class QueryNetworkDynamicHeads(nn.Module):\n",
        "    \"\"\"\n",
        "        Network for generating queries for attention\n",
        "\n",
        "        Params:\n",
        "        - c_k (int): Dimension of key vectors\n",
        "        - c_s (int): Dimension of spatial basis vectors\n",
        "        - num_queries (int): Number of attention heads\n",
        "    \"\"\"\n",
        "    def __init__(self, c_k, c_s, num_queries):\n",
        "        super(QueryNetworkDynamicHeads, self).__init__()\n",
        "        self.c_k = c_k\n",
        "        self.c_s = c_s\n",
        "        self.num_queries = num_queries\n",
        "\n",
        "        #Feedforward network for query generation\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(256, 128),  #First linear transformation\n",
        "            nn.LayerNorm(128),    #Normalisation for stability\n",
        "            nn.ReLU(),            #Activation for non linearity\n",
        "            nn.Linear(128, 128),  #Second linear transformation\n",
        "            nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1280),  #Last layer outputs 1280 features\n",
        "            nn.LayerNorm(1280),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        #Transformation layer reshapes the output to the expected dimensions\n",
        "        self.output_transform = nn.Linear(1280, (c_k + c_s) * num_queries)\n",
        "\n",
        "    def forward(self, query):\n",
        "        if print_logs:\n",
        "          print(f\"[QueryNetwork] Input query shape: {query.shape}\")\n",
        "        out = self.model(query)\n",
        "        out = self.output_transform(out)  # Transform output to match (c_k + c_s) * num_queries\n",
        "        out = out.reshape(-1, self.num_queries, self.c_k + self.c_s)  # Reshape for query output\n",
        "        if print_logs:\n",
        "          print(f\"[QueryNetwork] Output shape: {out.shape}\")\n",
        "        return out\n",
        "\n",
        "class SpatialBasisDynamicHeads:\n",
        "    \"\"\"\n",
        "        Class to generate spatial basis, which augments input features with spatial info\n",
        "\n",
        "        Params:\n",
        "        - channels (int): Number of output channels after applying spatial basis\n",
        "    \"\"\"\n",
        "    def __init__(self, channels=64):\n",
        "        self.channels = channels\n",
        "\n",
        "    def __call__(self, X):\n",
        "        batch_size, h, w, _ = X.size()\n",
        "\n",
        "        #Generating position encodings for height and width\n",
        "        p_h = torch.mul(torch.arange(1, h+1).unsqueeze(1).float(), torch.ones(1, w).float()) * (np.pi / h)\n",
        "        p_w = torch.mul(torch.ones(h, 1).float(), torch.arange(1, w+1).unsqueeze(0).float()) * (np.pi / w)\n",
        "\n",
        "        U = V = 8 #size of U, V\n",
        "        u_basis = v_basis = torch.arange(1, U+1).unsqueeze(0).float() #Size of basis vectors\n",
        "        a = torch.mul(p_h.unsqueeze(2), u_basis) #Positional encodings for U basis\n",
        "        b = torch.mul(p_w.unsqueeze(2), v_basis) #Positional encodings for V basis\n",
        "        out = torch.einsum('hwu,hwv->hwuv', torch.cos(a), torch.cos(b)).reshape(h, w, self.channels).to(X.device)\n",
        "\n",
        "        S = out.unsqueeze(0).expand(batch_size, -1, -1, -1) #Expand across batch\n",
        "        return torch.cat([X, S], dim=3) #Concatenate spatial basis with input tensor\n",
        "\n",
        "def spatial_softmaxDynamicHeads(A):\n",
        "    # A: batch_size x h x w x d\n",
        "    b, h, w, d = A.size()\n",
        "    # Flatten A s.t. softmax is applied to each grid (not over queries)\n",
        "    A = A.reshape(b, h * w, d)\n",
        "    A = F.softmax(A, dim=1)\n",
        "    # Reshape A to original shape.\n",
        "    A = A.reshape(b, h, w, d)\n",
        "    return A\n",
        "\n",
        "\n",
        "def apply_alphaDynamicHeads(A, V):\n",
        "    b, h, w, c = A.size()\n",
        "    A = A.reshape(b, h * w, c).transpose(1, 2)\n",
        "    _, _, _, d = V.size()\n",
        "    V = V.reshape(b, h * w, d)\n",
        "\n",
        "    return torch.matmul(A, V)\n",
        "\n",
        "class AgentDynamicHeads(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_actions,\n",
        "        hidden_size: int = 256,\n",
        "        c_v: int = 32,\n",
        "        c_k: int = 32,\n",
        "        c_s: int = 64,\n",
        "        num_queries: int = 4,\n",
        "    ):\n",
        "        \"\"\"Agent implementing the attention agent.\n",
        "        \"\"\"\n",
        "        super(AgentDynamicHeads, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.c_v, self.c_k, self.c_s, self.num_queries = c_v, c_k, c_s, num_queries\n",
        "\n",
        "        self.vision = VisionNetworkDynamicHeads()\n",
        "        self.query = QueryNetworkDynamicHeads(c_k=self.c_k, c_s=self.c_s, num_queries=self.num_queries)\n",
        "        self.spatial = SpatialBasisDynamicHeads()\n",
        "\n",
        "        self.answer_processor = nn.Sequential(\n",
        "            # 1026 x 512\n",
        "            nn.Linear(\n",
        "                (c_v + c_s) * num_queries + (c_k + c_s) * num_queries + 1 + 1, 512\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, hidden_size),\n",
        "        )\n",
        "\n",
        "        self.policy_core = nn.LSTMCell(hidden_size, hidden_size)\n",
        "        self.prev_output = None\n",
        "        self.prev_hidden = None\n",
        "\n",
        "        self.policy_head = nn.Sequential(nn.Linear(hidden_size, num_actions))\n",
        "        self.values_head = nn.Sequential(nn.Linear(hidden_size, 1))\n",
        "\n",
        "    def reset(self):\n",
        "        self.vision.reset()\n",
        "        self.prev_output = None\n",
        "        self.prev_hidden = None\n",
        "\n",
        "    def forward(self, X, prev_reward=None, prev_action=None):\n",
        "\n",
        "        #if print_logs:\n",
        "        #print(X)\n",
        "        #print(f\"Agent input shape: {X.shape}\")\n",
        "\n",
        "        #Convert input tensor to greyscale if RGB\n",
        "        if X.shape[1] == 3:\n",
        "            if print_model_logs : print(\"[Agent] Converting input tensor to grayscale\")\n",
        "            X = torch.mean(X, dim=1, keepdim=True)\n",
        "            if print_logs:\n",
        "              print(f\"Agent NEW input shape: {X.shape}\")\n",
        "\n",
        "        # 0. Setup.\n",
        "        # ---------\n",
        "        batch_size = X.size()[0]\n",
        "\n",
        "        if isinstance(prev_reward, np.ndarray):\n",
        "          prev_reward = torch.tensor(prev_reward).to(X.device)\n",
        "        if isinstance(prev_action, np.ndarray):\n",
        "          prev_action = torch.tensor(prev_action).to(X.device)\n",
        "\n",
        "\n",
        "\n",
        "        if prev_reward is None:\n",
        "            # (n, 1, 1)\n",
        "            prev_reward = torch.stack([torch.zeros(1, 1)] * batch_size).to(X.device)\n",
        "        else:\n",
        "            prev_reward = prev_reward.unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "        if prev_action is None:\n",
        "            # (n, 1, 1)\n",
        "            prev_action = torch.stack([torch.zeros(1, 1)] * batch_size).to(X.device)\n",
        "        else:\n",
        "            prev_action = prev_action.unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "        # 1 (a). Vision.\n",
        "        # --------------\n",
        "\n",
        "        # (n, h, w, c_k + c_v)\n",
        "        O = self.vision(X)\n",
        "\n",
        "\n",
        "        K, V = O.split([self.c_k, self.c_v], dim=3)\n",
        "        if print_logs:\n",
        "          print(f\"K SPLIT: {K.shape}\")\n",
        "          print(f\"V SPLIT: {V.shape}\")\n",
        "        #log_with_timestamp(f\"[Agent] Split K shape: {K.shape}, V shape: {V.shape}\", logs_dir_path+\"/debug.log\")\n",
        "        K, V = self.spatial(K), self.spatial(V)\n",
        "        #log_with_timestamp(f\"[Agent] After Spatial K shape: {K.shape}, V shape: {V.shape}\", logs_dir_path+\"/debug.log\")\n",
        "        if print_logs:\n",
        "          print(f\"K SPATIAL: {K.shape}\")\n",
        "          print(f\"V SPATIAL: {V.shape}\")\n",
        "\n",
        "        #Query Network\n",
        "        if self.prev_output is None or self.prev_output.shape[0] != batch_size:\n",
        "            self.prev_output = torch.zeros(batch_size, self.hidden_size, device=X.device, requires_grad=True)\n",
        "        Q = self.query(self.prev_output)\n",
        "        #print(f\"[Agent] Previous output shape: {self.prev_output.shape}\")\n",
        "\n",
        "        #log_with_timestamp(f\"[Agent] Vision output O shape: {O.shape}\", logs_dir_path+\"/debug.log\")\n",
        "        #K, V = O.split([self.c_k, self.c_v], dim=3)\n",
        "        #log_with_timestamp(f\"[Agent] Split K shape: {K.shape}, V shape: {V.shape}\", logs_dir_path+\"/debug.log\")\n",
        "        #K, V = self.spatial(K), self.spatial(V)\n",
        "        #log_with_timestamp(f\"[Agent] After Spatial K shape: {K.shape}, V shape: {V.shape}\", logs_dir_path+\"/debug.log\")\n",
        "\n",
        "        #print(f\"[Agent] Previous output shape: {self.prev_output.shape}\")\n",
        "\n",
        "        # 1 (b). Queries.\n",
        "        # --------------\n",
        "        if self.prev_output is None:\n",
        "            hidden_state = torch.zeros(\n",
        "                batch_size, self.hidden_size, requires_grad=True\n",
        "            ).to(X.device)\n",
        "            self.prev_output = hidden_state\n",
        "        # (n, h, w, num_queries, c_k + c_s)\n",
        "        Q = self.query(self.prev_output)\n",
        "\n",
        "        # 2. Answer.\n",
        "        # ----------\n",
        "        # (n, h, w, num_queries)\n",
        "        if print_logs:\n",
        "          print(f\"K shape: {K.shape}\")\n",
        "          print(f\"Q shape: {Q.shape}\")\n",
        "          print(f\"V shape: {V.shape}\")\n",
        "\n",
        "        #transposed_Q = Q.transpose(2, 1).unsqueeze(1)\n",
        "        #print(f\"transposed_Q shape: {transposed_Q.shape}\")\n",
        "\n",
        "        A = torch.matmul(K, Q.transpose(2, 1).unsqueeze(1))\n",
        "        # (n, h, w, num_queries)\n",
        "        if print_logs:\n",
        "          print(f\"A shape: {A.shape} - (n, h, w, num_queries)\")\n",
        "        A = spatial_softmaxDynamicHeads(A)\n",
        "        if print_logs:\n",
        "          print(f\"spatial_softmax(A): {A.shape} -(n, 1, 1, num_queries)\")\n",
        "        # (n, 1, 1, num_queries)\n",
        "        a = apply_alphaDynamicHeads(A, V)\n",
        "\n",
        "        # (n, (c_v + c_s) * num_queries + (c_k + c_s) * num_queries + 1 + 1)\n",
        "        answer = torch.cat(\n",
        "            torch.chunk(a, 4, dim=1)\n",
        "            + torch.chunk(Q, 4, dim=1)\n",
        "            + (prev_reward.float(), prev_action.float()),\n",
        "            dim=2,\n",
        "        ).squeeze(1)\n",
        "        # (n, hidden_size)\n",
        "        answer = self.answer_processor(answer)\n",
        "\n",
        "        # 3. Policy.\n",
        "        # ----------\n",
        "        if self.prev_hidden is None:\n",
        "            h, c = self.policy_core(answer)\n",
        "        else:\n",
        "            h, c = self.policy_core(answer, (self.prev_output, self.prev_hidden))\n",
        "            self.prev_output, self.prev_hidden = h, c\n",
        "        # (n, hidden_size)\n",
        "        output = h\n",
        "\n",
        "        # 4, 5. Outputs.\n",
        "        # --------------\n",
        "        # (n, num_actions)\n",
        "        logits = self.policy_head(output)\n",
        "        # (n, num_actions)\n",
        "        values = self.values_head(output)\n",
        "        if print_logs:\n",
        "          print(f\"logits: {logits.shape}\")\n",
        "          print(f\"values: {values.shape}\")\n",
        "        return logits, values, (h, c), A.split(1, dim=-1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O18E5jqI8CO9"
      },
      "source": [
        "##Vanilla IMPALA Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92_Di4448B09"
      },
      "outputs": [],
      "source": [
        "class VanillaImpalaCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VanillaImpalaCNN, self).__init__()\n",
        "        self.block1 = ImpalaBlock(in_channels=1, out_channels=16)\n",
        "        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n",
        "        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n",
        "        self.fc = nn.Linear(1568, 256)\n",
        "\n",
        "        self.critic = init_critic_(nn.Linear(256, 1))\n",
        "        self.actor = init_actor_(nn.Linear(256, n_actions))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(f\"Input shape: {x.shape}\")\n",
        "        x = self.block1(x)\n",
        "        #print(f\"Shape after block1: {x.shape}\")\n",
        "        x = self.block2(x)\n",
        "        #print(f\"Shape after block2: {x.shape}\")\n",
        "        x = self.block3(x)\n",
        "        #print(f\"Shape after block3: {x.shape}\")\n",
        "        x = nn.ReLU()(x)\n",
        "        x = Flatten()(x)\n",
        "        #print(f\"Shape after flatten: {x.shape}\")\n",
        "        x = self.fc(x)\n",
        "        #print(f\"Shape after fc: {x.shape}\")\n",
        "\n",
        "        x = nn.ReLU()(x)\n",
        "\n",
        "        c = self.critic(x)\n",
        "        a = nn.LogSoftmax(dim=-1)(self.actor(x))\n",
        "        return a, c, None, None"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cTKmCa1c9C1c"
      },
      "source": [
        "## IMPALA with Policy LSTM and Vision LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KOjSSDc9dR2"
      },
      "outputs": [],
      "source": [
        "class ImpalaPolicyLSTM_VisionLSTM(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hidden_size=256,\n",
        "                 num_queries=4,\n",
        "                 c_v: int = 16,\n",
        "                 c_k: int = 16,\n",
        "                 c_s: int = 64,):\n",
        "        super(ImpalaPolicyLSTM_VisionLSTM, self).__init__()\n",
        "        self.block1 = ImpalaBlock(in_channels=1, out_channels=16)\n",
        "        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n",
        "        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n",
        "        self.fc = nn.Linear(256, 256)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.c_v = c_v\n",
        "        self.c_k = c_k\n",
        "        self.c_s = c_s\n",
        "        self.num_queries = num_queries\n",
        "\n",
        "        self.critic = init_critic_(nn.Linear(256, 1))\n",
        "        self.actor = init_actor_(nn.Linear(256, n_actions))\n",
        "\n",
        "        self.vision_lstm = ConvLSTMCell(\n",
        "            input_channels=32,\n",
        "            hidden_channels=32,\n",
        "            kernel_size=3\n",
        "        )\n",
        "\n",
        "        self.answer_processor = nn.Sequential(\n",
        "            # 1026 x 512\n",
        "            nn.Linear(\n",
        "                (c_v + c_s) * num_queries + (c_k + c_s) * num_queries + 1 + 1, 512\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, hidden_size),\n",
        "        )\n",
        "\n",
        "        self.spatial = SpatialBasis()\n",
        "        self.query = QueryNetwork(c_k, c_s, num_queries)\n",
        "\n",
        "        self.prev_output = None\n",
        "        self.prev_hidden = None\n",
        "        self.policy_core = nn.LSTMCell(hidden_size, hidden_size) # Core LSTM for policy decision-making\n",
        "\n",
        "        #Initialises attention head mask\n",
        "        self.attention_mask = torch.ones(num_queries, dtype=torch.bool)\n",
        "\n",
        "    def set_attention_mask(self, mask):\n",
        "        #Sets mask for attention heads\n",
        "        self.attention_mask = mask\n",
        "\n",
        "\n",
        "    def forward(self, x, prev_reward=None, prev_action=None):\n",
        "\n",
        "        batch_size = x.size()[0]\n",
        "        self.attention_mask = self.attention_mask.to(x.device)\n",
        "\n",
        "        if prev_reward is None:\n",
        "            # (n, 1, 1)\n",
        "            prev_reward = torch.stack([torch.zeros(1, 1)] * batch_size).to(x.device)\n",
        "        else:\n",
        "            prev_reward = torch.tensor(prev_reward).clone().detach().to(device)\n",
        "            prev_reward = prev_reward.unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "        if prev_action is None:\n",
        "            # (n, 1, 1)\n",
        "            prev_action = torch.stack([torch.zeros(1, 1)] * batch_size).to(x.device)\n",
        "        else:\n",
        "            prev_action = torch.tensor(prev_action).clone().detach().to(device)\n",
        "            prev_action = prev_action.unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "        #print(f\"Input shape: {x.shape}\")\n",
        "        x = self.block1(x)\n",
        "        #print(f\"Shape after block1: {x.shape}\")\n",
        "        x = self.block2(x)\n",
        "        #print(f\"Shape after block2: {x.shape}\")\n",
        "        x = self.block3(x)\n",
        "        #print(f\"Shape after block3: {x.shape}\")\n",
        "        x = nn.ReLU()(x)\n",
        "\n",
        "        x, _ = self.vision_lstm(x)\n",
        "        '''\n",
        "        x = Flatten()(x)\n",
        "        #print(f\"Shape after flatten: {x.shape}\")  # Print shape after flatten\n",
        "        x = self.fc(x)\n",
        "        #print(f\"Shape after fc: {x.shape}\")  # Print shape after fc\n",
        "\n",
        "\n",
        "        x = nn.ReLU()(x)\n",
        "\n",
        "        '''\n",
        "        x = x.transpose(1, 3)\n",
        "\n",
        "        batch_size = x.size()[0]\n",
        "\n",
        "\n",
        "        if self.prev_output is None or self.prev_output.shape[0]!=batch_size:\n",
        "            hidden_state = torch.zeros(\n",
        "                batch_size, self.hidden_size, requires_grad=True\n",
        "            ).to(x.device)\n",
        "            self.prev_output = hidden_state\n",
        "\n",
        "         # 3. Policy core LSTM\n",
        "        # ----------\n",
        "        if self.prev_hidden is None:\n",
        "            h, c = self.policy_core(x)\n",
        "        else:\n",
        "            h, c = self.policy_core(x, (self.prev_output, self.prev_hidden))\n",
        "            self.prev_output, self.prev_hidden = h, c\n",
        "        # (n, hidden_size)\n",
        "        x = h\n",
        "\n",
        "        #x = nn.ReLU()(x)\n",
        "\n",
        "        c = self.critic(x) # state value\n",
        "        a = nn.LogSoftmax(dim=-1)(self.actor(x))  # action logits\n",
        "\n",
        "        return a, c, (h, c), None #Attention passed out so we can visualise it\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wAZ9xraC_6PX"
      },
      "source": [
        "## IMPALA Policy LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnGUNCwm_5LJ"
      },
      "outputs": [],
      "source": [
        "class ImpalaPolicyLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size=256):\n",
        "        super(ImpalaPolicyLSTM, self).__init__()\n",
        "        self.block1 = ImpalaBlock(in_channels=1, out_channels=16)\n",
        "        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n",
        "        self.block3 = ImpalaBlock(in_channels=32, out_channels=32)\n",
        "        self.fc = nn.Linear(6272, 256)\n",
        "\n",
        "        self.vision_lstm = ConvLSTMCell(\n",
        "            input_channels=32,\n",
        "            hidden_channels=128,\n",
        "            kernel_size=3\n",
        "        )\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.prev_output = None\n",
        "        self.prev_hidden = None\n",
        "        self.policy_core = nn.LSTMCell(hidden_size, hidden_size) #Core LSTM for policy decision-making\n",
        "\n",
        "        self.critic = init_critic_(nn.Linear(256, 1))\n",
        "        self.actor = init_actor_(nn.Linear(256, n_actions))\n",
        "\n",
        "    def forward(self, x, prev_reward=None, prev_action=None):\n",
        "\n",
        "        batch_size = x.size()[0]\n",
        "        if self.prev_output is None or self.prev_output.shape[0]!=batch_size:\n",
        "            hidden_state = torch.zeros(\n",
        "                batch_size, self.hidden_size, requires_grad=True\n",
        "            ).to(x.device)\n",
        "            self.prev_output = hidden_state\n",
        "\n",
        "        #print(f\"Model input type: {type(x)}\")\n",
        "        #print(f\"Input shape: {x.shape}\")  # Print input shape\n",
        "        x = self.block1(x)\n",
        "        #print(f\"Shape after block1: {x.shape}\")  # Print shape after block1\n",
        "        x = self.block2(x)\n",
        "        #print(f\"Shape after block2: {x.shape}\")  # Print shape after block2\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x, _ = self.vision_lstm(x)\n",
        "\n",
        "        #print(f\"Shape after block3: {x.shape}\")  # Print shape after block3\n",
        "        x = nn.ReLU()(x)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        #print(f\"Shape after flatten: {x.shape}\")  # Print shape after flatten\n",
        "        x = self.fc(x)\n",
        "        #print(f\"Shape after fc: {x.shape}\")  # Print shape after fc\n",
        "\n",
        "        x = nn.ReLU()(x)\n",
        "\n",
        "         # 3. Policy core LSTM\n",
        "        # ----------\n",
        "        if self.prev_hidden is None:\n",
        "            h, c = self.policy_core(x)\n",
        "        else:\n",
        "            h, c = self.policy_core(x, (self.prev_output, self.prev_hidden))\n",
        "            self.prev_output, self.prev_hidden = h, c\n",
        "        # (n, hidden_size)\n",
        "        x = h\n",
        "\n",
        "        c = self.critic(x)\n",
        "        a = nn.LogSoftmax(dim=-1)(self.actor(x))\n",
        "        return a, c, None, None"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ggqRE0Bla5n1"
      },
      "source": [
        "## Agent Configuration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yG3hNazx6quO"
      },
      "source": [
        "Various different models to be tested"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ACApmQdsE3sw",
        "outputId": "019380d6-8c44-4fde-f766-a11307bb69c3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.853 M params'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#agent = VanillaImpalaCNN().to(device)\n",
        "#agent = ImpalaPolicyLSTM().to(device)\n",
        "#agent = ImpalaPolicyLSTM_VisionLSTM().to(device)\n",
        "agent = AttentionImpala().to(device)\n",
        "#agent = AgentDynamicHeads(num_actions=n_actions).to(device)\n",
        "is_attention_agent = False\n",
        "get_n_params(agent)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh7ciFj2a5n2"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_5TLI5UH2Pj",
        "outputId": "4dfc6b90-99ea-44c0-899b-4a25ec6d2ead"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(210, 160, 3)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.observation_space.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xktgDPnkwB2b"
      },
      "outputs": [],
      "source": [
        "resized_obs = resize_observations(observations, new_shape=(84, 84))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3FX9JPQE3s9",
        "outputId": "ee975ea5-be03-42de-d8b3-d3bc8aa9ba5e",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(72, 84, 84, 1)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# take random first step in our envs\n",
        "env.reset()\n",
        "frame, reward, done, info = env.step(np.array([env.action_space.sample() for _ in range(n_envs)]));\n",
        "frame = resize_observations(frame, new_shape=(84, 84))\n",
        "frame.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "C4msy7YpE3tE",
        "outputId": "a68dd135-8eeb-44e7-ba6c-292916551e21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7bfd167a2440>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(frame[0], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5pCYBzsE3tH",
        "outputId": "c3c2e56a-e305-4e20-a2e1-a6e1fbca62ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([72, 1, 84, 84])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "frame = np_to_pytorch_img(frame).to(device)\n",
        "frame.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u0XptmjaG4tF"
      },
      "source": [
        "# Optimiser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DVGEktsG6c9"
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.Adam(agent.parameters(), lr=lr, eps=1e-5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h5AUn-gZpt90"
      },
      "source": [
        "# Actor Critic with PPO\n",
        "- Gathering trajectories\n",
        "- Learning from the trajectories"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LtcXgRG-qSuy"
      },
      "source": [
        "### Generalised Advantage Estimation (GAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAptYy-fE3tm",
        "outputId": "9d7b588e-e82d-4096-e362-eb3f15c50f8e",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def compute_returns_gae(rewards, dones, values):\n",
        "    \"\"\" Compute discounted returns w Generalized Advantage Estimation (GAE) \"\"\"\n",
        "\n",
        "    #print(f\"rewards shape: {rewards.shape}\")\n",
        "    #print(f\"rewards type: {type(rewards)}\")\n",
        "    #print(f\"dones shape: {dones.shape}\")\n",
        "    #print(f\"rewards type: {type(rewards)}\")\n",
        "    #print(f\"values shape: {values.shape}\")\n",
        "    #print(f\"rewards type: {type(rewards)}\")\n",
        "\n",
        "    GAMMA = .999\n",
        "    if is_dynamic_heads_model:\n",
        "      LAM = 1.0 # Setting from dynamic heads paper\n",
        "    else:\n",
        "      LAM = .95 # Adjusting to this to make use of GAE. Setting lamdba to 1 means that all future rewards are calculated equally, as with standard Advantage function\n",
        "\n",
        "    lastgaelam = 0\n",
        "    A = [np.zeros(rewards.shape[-1])]\n",
        "    for t in reversed(range(0, len(rewards)-1)):\n",
        "        nextnonterminal = 1.0 - dones[t]\n",
        "        nextvalues = values[t+1]\n",
        "        delta = rewards[t] + GAMMA * nextvalues * nextnonterminal - values[t]\n",
        "        lastgaelam = delta + GAMMA * LAM * nextnonterminal * lastgaelam\n",
        "        A.append(lastgaelam)\n",
        "    A.reverse()\n",
        "    A = np.array(A)\n",
        "    R = A + values\n",
        "    return R\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Im0PvDfTa5oA"
      },
      "source": [
        "## Gather Trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN5-48wIE3tq"
      },
      "outputs": [],
      "source": [
        "def gather_trajectories():\n",
        "    with torch.no_grad():\n",
        "        agent.eval()\n",
        "        global last_frame_\n",
        "        frame = last_frame_\n",
        "        #print(f\"frame: {frame.shape}\")\n",
        "        frames_, rewards_, dones_, actions_, old_action_probs_, old_state_estimates_, epinfos = [], [], [], [], [], [], []\n",
        "        attention_entropies = {1: [], 2: [], 3: [], 4: []}  #Dict to store entropy values for each attention head\n",
        "\n",
        "        prev_action = None\n",
        "        prev_reward = None\n",
        "\n",
        "        for s in range(n_steps):\n",
        "            #frames_.append(frame);\n",
        "            frames_.append(resize_observations(frame, new_shape=(64, 64)));\n",
        "            #resize_observations(last_frame_, new_shape=(64, 64))\n",
        "\n",
        "            if is_attention_agent:\n",
        "              action_probs, state_estimate, hidden_state, attention_maps = agent(np_to_pytorch_img(frame).to(device), prev_reward, prev_action);\n",
        "              if not hidden_state == None:\n",
        "                hx, cx = hidden_state\n",
        "                history_hx.append(hx.squeeze(0).cpu().data.numpy())\n",
        "                history_cx.append(cx.squeeze(0).cpu().data.numpy())\n",
        "            else:\n",
        "              action_probs, state_estimate, hidden_state, attention_maps = agent(np_to_pytorch_img(frame).to(device));\n",
        "\n",
        "\n",
        "            action = get_action(action_probs).cpu().numpy();\n",
        "            frame, reward, done, info = env.step(action)\n",
        "            frame = resize_observations(frame, new_shape=(64, 64))\n",
        "            rewards_.append(reward); dones_.append(done); actions_.append(action); old_action_probs_.append(action_probs.detach().cpu().numpy()); old_state_estimates_.append(state_estimate.detach().cpu().numpy())\n",
        "\n",
        "            prev_action = action\n",
        "            prev_reward = reward\n",
        "\n",
        "            for i in info:\n",
        "                episode_info = i.get('episode')\n",
        "                if episode_info: epinfos.append(episode_info)\n",
        "\n",
        "            if is_attention_agent and not attention_maps == None:\n",
        "              for i, attention_map in enumerate(attention_maps, 1):\n",
        "\n",
        "                  #print(f\"i : {i}\")\n",
        "                  attention_map_np = attention_map.cpu().data.numpy()\n",
        "                  entropy = calculate_entropy(attention_map_np)\n",
        "                  #print(f\"attention_map_np: {attention_map_np.shape}\")\n",
        "                  #Attention weights for each head\n",
        "                  #Append attention weights to corresponding attention head list\n",
        "                  if i == 1:\n",
        "                      attention_head_1.append(attention_map.cpu().data.numpy())\n",
        "                      #print(f\"attention_map {attention_map.cpu().shape}\")\n",
        "                      attention_entropies[1].append(entropy)\n",
        "                  elif i == 2:\n",
        "                      attention_head_2.append(attention_map.cpu().data.numpy())\n",
        "                      attention_entropies[2].append(entropy)\n",
        "                  elif i == 3:\n",
        "                      attention_head_3.append(attention_map.cpu().data.numpy())\n",
        "                      attention_entropies[3].append(entropy)\n",
        "                  elif i == 4:\n",
        "                      attention_head_4.append(attention_map.cpu().data.numpy())\n",
        "                      attention_entropies[4].append(entropy)\n",
        "\n",
        "        rewards_ = np.array(rewards_); dones_ = np.array(dones_); actions_ = np.array(actions_); frames_ = np.array(frames_); old_state_estimates_ = np.array(old_state_estimates_);\n",
        "\n",
        "        last_frame_ = frames_[-1] # reset global last frame. Next time we gather trajectories we'll pick up here\n",
        "\n",
        "        rewards_[-1] = state_estimate.squeeze(-1).cpu().numpy() # Bootstrapped returns\n",
        "\n",
        "        returns_ = compute_returns_gae(rewards_, dones_, old_state_estimates_.squeeze(-1))\n",
        "\n",
        "        # Reshaping dims and prepping tensor types and locations. Nothing conceptually interesting.\n",
        "        returns_, old_state_estimates_, old_action_probs_, actions_, frames_ = reshaping_processing_acrobatics(returns_, old_state_estimates_, old_action_probs_, actions_, frames_)\n",
        "\n",
        "        return returns_, frames_, actions_, old_action_probs_, old_state_estimates_, epinfos, attention_entropies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YQaOaMuzDOy"
      },
      "outputs": [],
      "source": [
        "last_frame_ = env.reset(); last_frame_.shape #(n_envs, height, width, channels)\n",
        "last_frame_ = resize_observations(last_frame_, new_shape=(64, 64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1Xot0J7y5hW",
        "outputId": "6dca6842-2f7b-4635-c11d-4bc898436b18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([18432]), torch.Size([18432, 1, 84, 84]), torch.Size([18432]))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Creating a global database of rollouts\n",
        "returns, frames, actions, old_action_probs, old_state_estimates, epinfos, _ = gather_trajectories()\n",
        "returns.shape, frames.shape, actions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKFsTIUkE3t6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def get_batch(batch_ix):\n",
        "    return frames[batch_ix].to(device), returns[batch_ix].to(device), actions[batch_ix].to(device), old_action_probs[batch_ix].to(device), old_state_estimates[batch_ix].to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dObRJJvmyoob"
      },
      "source": [
        "## Action Gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc-mqbqBE3t9"
      },
      "outputs": [],
      "source": [
        "def calculate_action_gain_ppo(action_probs, old_action_probs, actions_taken, advantages, verbose=False):\n",
        "    \"\"\" Actions resulting in positive advantage made more probable, and vice versa. Do not allow to differ from old\n",
        "    probs by too much. Log probs are from current agent. Actions_taken are from OLD version of agent. \"\"\"\n",
        "\n",
        "    a = actions_taken.unsqueeze(-1).to(device)\n",
        "    if verbose: print(\"\\n\\nactions unsqueezed\\n\", a)\n",
        "\n",
        "    chosen_action_probs = action_probs.gather(1, a)\n",
        "    chosen_action_probs = chosen_action_probs.squeeze(-1)\n",
        "\n",
        "    old_chosen_action_probs = old_action_probs.gather(1, a)\n",
        "    old_chosen_action_probs = old_chosen_action_probs.squeeze(-1)\n",
        "    if verbose: print(\"\\n\\nchosen action probs, new and old\\n\", chosen_action_probs,'\\n', old_chosen_action_probs, '\\n\\nadvantages values', advantages)\n",
        "\n",
        "    ratio = torch.exp(chosen_action_probs - old_chosen_action_probs)\n",
        "    if verbose: print('\\n\\nratio', ratio)\n",
        "\n",
        "    unclipped_action_gain = ratio * advantages\n",
        "    clipped_action_gain = torch.clamp(ratio, .8, 1.2) * advantages\n",
        "    if verbose: print('\\n\\nunclipped and clipped action gains\\n', unclipped_action_gain, '\\n', clipped_action_gain)\n",
        "\n",
        "    action_gain = torch.min(unclipped_action_gain, clipped_action_gain)\n",
        "    if verbose: print('\\n\\n conservative lower bound action gain\\n', action_gain)\n",
        "\n",
        "    action_gain = action_gain.mean()\n",
        "    if verbose: print('\\n\\nmean', action_gain)\n",
        "\n",
        "    return action_gain # single scalar"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7n1aSB2l30wd"
      },
      "source": [
        "## Critic Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmZBSqWQE3uG"
      },
      "outputs": [],
      "source": [
        "def get_critic_loss(old_state_estimates_b, state_estimates, returns_b, verbose=False):\n",
        "    \"\"\" How good is critic at estimating state? Don't allow to differ too much from old state estimates \"\"\"\n",
        "\n",
        "    state_estimates = state_estimates.squeeze(-1)\n",
        "    clipped_state_estimate = old_state_estimates_b + torch.clamp(state_estimates - old_state_estimates_b, -.2, .2)\n",
        "    if verbose: print(\"\\nstate estimates, new and old\\n\\n\", state_estimates, '\\n', old_state_estimates_b, '\\n\\nreturns', returns_b)\n",
        "    critic_loss_1 = ((returns_b - clipped_state_estimate)**2)\n",
        "    critic_loss_2 = ((returns_b - state_estimates)**2)\n",
        "    critic_loss = torch.max(critic_loss_1, critic_loss_2)\n",
        "    if verbose: print('\\nCritic Losses: clipped, unclipped, conservative:\\n', critic_loss_1, '\\n', critic_loss_2, '\\n', critic_loss)\n",
        "    critic_loss = critic_loss.mean() * .5\n",
        "    return critic_loss # single scalar"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm_8YqwzH9fY"
      },
      "source": [
        "## Entropy Term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj55j_gJE3uK"
      },
      "outputs": [],
      "source": [
        "def get_entropy_bonus(action_probs):\n",
        "    e = -(action_probs.exp() * (action_probs+1e-8))\n",
        "    e = e.sum(dim=1)\n",
        "    e = e.mean()\n",
        "    return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQF4Zv9aE3uS"
      },
      "outputs": [],
      "source": [
        "def run_batch(batch_ix, verbose=False, prev_reward=None, prev_action=None):\n",
        "    \"\"\"\n",
        "    Run a single batch of data. Takes in indices, uses them to pull from global database of rollouts,\n",
        "    calculates and returns losses. No gradient steps here.\n",
        "    \"\"\"\n",
        "\n",
        "    frames_b, returns_b, actions_taken_b, old_action_probs_b, old_state_estimates_b = get_batch(batch_ix)\n",
        "\n",
        "    action_probs, state_estimates, _, _ = agent(frames_b)\n",
        "    entropy_bonus = get_entropy_bonus(action_probs)\n",
        "    critic_loss = get_critic_loss(old_state_estimates_b, state_estimates, returns_b, verbose=verbose)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        advantages = (returns_b - state_estimates.squeeze(-1).detach()) # don't want to propogate gradients through this\n",
        "        advantages -= advantages.mean()\n",
        "        advantages /= (advantages.std() + 1e-8)\n",
        "\n",
        "    action_gain = calculate_action_gain_ppo(action_probs, old_action_probs_b, actions_taken_b, advantages, verbose=verbose)\n",
        "\n",
        "    return entropy_bonus, action_gain, critic_loss, frames_b, returns_b, actions_taken_b"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SA6FsH8cqgaz"
      },
      "source": [
        "## Learn from Current Trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrnnZfk4E3ub",
        "outputId": "d48e3835-c487-476d-e78b-60605b660eb9",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.2185134571045637"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def reflect(verbose=False):\n",
        "    \"\"\" Learn from current database of rollouts for a single epoch \"\"\"\n",
        "\n",
        "    agent.train()\n",
        "    epoch_losses = []\n",
        "    ix_range = range(len(returns))\n",
        "    ix = 0\n",
        "    grad_accum_counter = 1\n",
        "\n",
        "    prev_action = None\n",
        "    prev_reward = None\n",
        "\n",
        "    while ix < len(returns):\n",
        "        batch_ix = ix_range[ix:ix+bs]; ix += bs\n",
        "        entropy_bonus, action_gain, critic_loss, frames_b, prev_reward, prev_action = run_batch(batch_ix, verbose=verbose, prev_reward=prev_reward, prev_action=prev_action)\n",
        "        entropy_bonus *= entropy_coef\n",
        "        critic_loss *= value_loss_coef\n",
        "        if verbose: print(\"\\n\\nentropy bonus, action gain\\n, critic loss\\n\", entropy_bonus.item(), action_gain.item(), critic_loss.item())\n",
        "        total_loss = critic_loss - entropy_bonus - action_gain\n",
        "        total_loss /= accumulation_steps\n",
        "        total_loss.backward()\n",
        "\n",
        "        if grad_accum_counter % accumulation_steps == 0:\n",
        "            nn.utils.clip_grad_norm_(agent.parameters(), .5);\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        grad_accum_counter+=1\n",
        "        epoch_losses.append(total_loss.item())\n",
        "        entropies.append(entropy_bonus.item())\n",
        "        entropy_coefs.append(entropy_coef)\n",
        "        policy_losses.append(action_gain.item())\n",
        "        value_losses.append(critic_loss.item())\n",
        "        a2c_losses.append(total_loss.item())\n",
        "\n",
        "        #Log to CSV\n",
        "        log_to_csv(len(epoch_losses), entropy_bonus.item(), action_gain.item(), critic_loss.item(), total_loss.item(), None, None)\n",
        "\n",
        "    return np.array(epoch_losses).mean(); # epoch avg loss\n",
        "\n",
        "reflect(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7EWGElDa5oL"
      },
      "outputs": [],
      "source": [
        "def shuffle_database():\n",
        "    \"\"\" Shuffle the global database of rollouts in place. Doesn't return anything. Requires that database exist. \"\"\"\n",
        "    global returns, frames, actions, old_action_probs, old_state_estimates\n",
        "    dataset_ix = torch.randperm(len(returns));\n",
        "    returns = returns[dataset_ix]\n",
        "    frames = frames[dataset_ix]\n",
        "    actions = actions[dataset_ix]\n",
        "    old_action_probs = old_action_probs[dataset_ix]\n",
        "    old_state_estimates = old_state_estimates[dataset_ix]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YGRH8qzG6veu"
      },
      "source": [
        "# Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoOQG-W2BRLI"
      },
      "outputs": [],
      "source": [
        "if save_attention_map_before_training:\n",
        "    if is_attention_agent:\n",
        "      generate_attention_maps(env_name,\n",
        "                              first_frame=100,\n",
        "                              num_frames=400,\n",
        "                              resolution=50,\n",
        "                              save_dir=logs_dir_path + \"/saved_attention_videos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "Dq1nhwUEE3ue",
        "outputId": "6bef0cf9-71a6-483a-9a24-95b639cd85c7",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Env Name: Breakout-v4\n",
            "\n",
            "Vanilla IMPALA Model\n",
            "\n",
            "Number of steps per trajectory: 256\n",
            "Number of trajectories per round: 72\n",
            "Total steps per round: 18432\n",
            "\n",
            "Total number of rounds: 1\n",
            "\n",
            "Total number of steps: 20000\n",
            "\n",
            "Entropy Coef: 0.01\n",
            "Value Loss Coef: 0.5\n",
            "LR: 0.0005\n",
            "\n",
            "\n",
            "Entropy Decay type: logarithmic\n",
            "Start decay: 0.01\n",
            "End decay: 0.001\n",
            "\n",
            "\n",
            "Entropy Coef: 0.010000000000000002\n",
            "\n",
            "Round 0\n",
            "Steps so far: 0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-144c865ecde7>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_opt_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mshuffle_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreflect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-dade3fb639c5>\u001b[0m in \u001b[0;36mreflect\u001b[0;34m(verbose)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mentropy_bonus\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0maction_gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrad_accum_counter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "head1_entropies, head2_entropies, head3_entropies, head4_entropies = [], [], [], []\n",
        "\n",
        "scores = []; losses = [];\n",
        "\n",
        "steps_so_far = 0\n",
        "last_save_frame = 0\n",
        "print(f\"Env Name: {env_name}\\n\")\n",
        "#print(f\"n_opt_epochs: {n_opt_epochs}\\n\" )\n",
        "\n",
        "if is_dynamic_heads_model:\n",
        "  print(\"Dynamic Heads Model\\n\")\n",
        "\n",
        "if not is_attention_agent and not is_dynamic_heads_model:\n",
        "  print(\"Vanilla IMPALA Model\\n\")\n",
        "else:\n",
        "  print(\"Attention Augmented Model\")\n",
        "  print(f\"Attention maps printed every {save_attention_map_interval} frames\\n\")\n",
        "\n",
        "print(f\"Number of steps per trajectory: {n_steps}\" )\n",
        "print(f\"Number of trajectories per round: {n_envs}\")\n",
        "print(f\"Total steps per round: {n_obs_per_round}\\n\" )\n",
        "print(f\"Total number of rounds: {n_rounds}\\n\" )\n",
        "\n",
        "print(f\"Total number of steps: {target_n_obs}\\n\")\n",
        "\n",
        "\n",
        "print(f\"Entropy Coef: {entropy_coef}\")\n",
        "print(f\"Value Loss Coef: {value_loss_coef}\")\n",
        "print(f\"LR: {lr}\")\n",
        "print(f\"\\n\")\n",
        "\n",
        "if enable_entropy_decay:\n",
        "  print(f\"Entropy Decay type: {entropy_decay_type}\")\n",
        "  print(f\"Start decay: {entropy_coef_start}\")\n",
        "  print(f\"End decay: {entropy_coef_end}\")\n",
        "  print(f\"\\n\")\n",
        "\n",
        "for i in range(n_rounds):\n",
        "\n",
        "    if enable_entropy_decay:\n",
        "      entropy_coef = adjust_entropy_coef(entropy_coef, i, n_rounds)\n",
        "      print(f\"Entropy Coef: {entropy_coef}\")\n",
        "\n",
        "    print(f\"\\nRound {i}\")\n",
        "\n",
        "    attention_head_1, attention_head_2, attention_head_3, attention_head_4 =[],[],[],[]\n",
        "    #history_hx, history_cx = [], []\n",
        "\n",
        "    steps_so_far = n_obs_per_round * i\n",
        "    print(f\"Steps so far: {steps_so_far}\")\n",
        "    with torch.no_grad():\n",
        "        returns, frames, actions, old_action_probs, old_state_estimates, epinfos, attention_entropies = gather_trajectories() # refresh 'database'\n",
        "        avg_score = get_avg_score(epinfos)\n",
        "        scores.append(avg_score)\n",
        "\n",
        "        if is_attention_agent:\n",
        "\n",
        "          mean_entropies = calculate_mean_entropies(attention_entropies)\n",
        "\n",
        "          head1_entropies.append(mean_entropies[1])\n",
        "          head2_entropies.append(mean_entropies[2])\n",
        "          head3_entropies.append(mean_entropies[3])\n",
        "          head4_entropies.append(mean_entropies[4])\n",
        "\n",
        "          #save_entropies(mean_entropies)\n",
        "\n",
        "          #Check if steps_so_far has crossed a multiple of 240000 since last save\n",
        "          if steps_so_far // save_attention_map_interval > last_save_frame // save_attention_map_interval:\n",
        "              #Update last save frame\n",
        "              last_save_frame = steps_so_far\n",
        "              save_path = os.path.join(logs_dir_path, \"saved_attention\")\n",
        "              os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "              generate_attention_maps(env_name,\n",
        "                                          first_frame=100,\n",
        "                                          num_frames=400,\n",
        "                                          resolution=50,\n",
        "                                          save_dir=save_path,\n",
        "                                          current_frame=steps_so_far)\n",
        "              print(f\"\\n Generated Attention Maps\\n\")\n",
        "\n",
        "    for e in range(n_opt_epochs):\n",
        "        shuffle_database()\n",
        "        loss = reflect(verbose = False)\n",
        "        losses.append(loss)\n",
        "\n",
        "    update_last_csv_line(avg_score, loss)\n",
        "\n",
        "    #if i % 100 == 0: print(i, avg_score)\n",
        "    print(i, avg_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l98cCvTzE3uf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "torch.save(agent.state_dict(), logs_dir_path + \"/\" + env_name +\"_agent.torch\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c-6zCeDroIGf"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2jFyyVYr2Q6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "#Generate x axis values based on the total number of obs\n",
        "x_values = [i * n_obs_per_round for i in range(len(scores))]\n",
        "\n",
        "plt.plot(x_values, scores)\n",
        "plt.title(\"Rewards \" + env_name)\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Returns\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud1d3tzdE3uh"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(len(scores)), scores)\n",
        "plt.title(\"Scores \" + env_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tqZaMXCDa5oN"
      },
      "source": [
        "### Manually Inspect Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhg4-NIuE3uo"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    returns, frames, actions, old_action_probs, old_state_estimates, epinfos, attention_entropies = gather_trajectories()\n",
        "\n",
        "returns.shape, frames.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FFDhRjjzlp2T"
      },
      "source": [
        "### Generate Attention Maps After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKb7B3BKlvLY"
      },
      "outputs": [],
      "source": [
        "#attention_mask = torch.tensor([True, True, True, True, True, True, True, True], dtype=torch.bool)\n",
        "#set_attention_mask(attention_mask)\n",
        "if is_attention_agent:\n",
        "  generate_attention_maps(env_name,\n",
        "                          first_frame=100,\n",
        "                          num_frames=400,\n",
        "                          resolution=50,\n",
        "                          save_dir=logs_dir_path + \"/after_training_saved_attention_videos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPJmwlUla5oO"
      },
      "outputs": [],
      "source": [
        "n_vid_steps = 1000"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WBIPzfutoPyb"
      },
      "source": [
        "### Actual Returns vs Estimated Returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q68ytRIRE3uq"
      },
      "outputs": [],
      "source": [
        "if is_attention_agent:\n",
        "  plt.figure(figsize=(20,5))\n",
        "  plt.plot(range(len(returns[:n_vid_steps])), returns[:n_vid_steps])\n",
        "  plt.plot(range(len(returns[:n_vid_steps])), old_state_estimates[:n_vid_steps])\n",
        "  plt.title(\"Actual Returns vs Estimated Returns\")\n",
        "  plt.legend([\"Actual\", \"Estimated\"])\n",
        "  plt.savefig(os.path.join(logs_dir_path, \"actual_v_estimated_returns.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NroyKUCXR4v"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "frames_np = frames[:n_vid_steps].numpy()\n",
        "returns_vid = returns[:n_vid_steps].numpy()\n",
        "\n",
        "returns_vid -= returns_vid.min()\n",
        "returns_vid /= returns_vid.max()\n",
        "\n",
        "scale_factor = 20\n",
        "height, width, layers = 84, 84, 3 # Original frame dimensions and number of color channels\n",
        "new_height, new_width = height * scale_factor, width * scale_factor # Scaled dimensions\n",
        "fps = 10\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(new_width/100, new_height/100), dpi=100)\n",
        "\n",
        "def update_frame(i):\n",
        "    frame = frames_np[i]\n",
        "    frame = np.transpose(frame, (1, 2, 0))\n",
        "    frame = np.uint8(frame * 255)\n",
        "    return_value = returns_vid[i]\n",
        "\n",
        "    frame[:5, :5, :] = 255 * return_value\n",
        "\n",
        "    frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    ax.clear()\n",
        "    ax.imshow(frame)\n",
        "    ax.axis('off')\n",
        "\n",
        "ani = animation.FuncAnimation(fig, update_frame, frames=n_vid_steps, interval=1000/fps)\n",
        "\n",
        "video_filename = logs_dir_path + \"/\" +env_name + '_agent_run.mp4'\n",
        "ani.save(video_filename, writer='ffmpeg', fps=fps)\n",
        "\n",
        "plt.close(fig)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SQUdyLqTGcdi"
      },
      "source": [
        "### Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3KOXczI841Y"
      },
      "outputs": [],
      "source": [
        "train_step_interval = [i * n_obs_per_round for i in range(1, len(scores) + 1)]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(train_step_interval, head1_entropies, label='Head 1')\n",
        "plt.plot(train_step_interval, head2_entropies, label='Head 2')\n",
        "plt.plot(train_step_interval, head3_entropies, label='Head 3')\n",
        "plt.plot(train_step_interval, head4_entropies, label='Head 4')\n",
        "\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Attetnion head Entropy')\n",
        "plt.title('Entropies of Attention Heads Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(logs_dir_path, \"attention_head_entropies.png\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6a3tqTwbTXf"
      },
      "outputs": [],
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "train_step_interval = [i * n_obs_per_round for i in range(1, len(scores) + 1)]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(train_step_interval, scores)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Training Reward')\n",
        "plt.title('Training Reward over Time')\n",
        "plt.savefig(os.path.join(logs_dir_path, \"training_reward.png\"))\n",
        "plt.show()\n",
        "'''\n",
        "evaluation_steps = [i * evaluation_step_interval for i in range(1, len(eval_rewards) + 1)]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(evaluation_steps, eval_rewards)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Evaluation Reward')\n",
        "plt.title('Evaluation Reward over Time')\n",
        "plt.savefig(os.path.join(logs_dir_path, \"evaluation_reward.png\"))\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "#plt.plot(train_step_interval, entropies)\n",
        "plt.plot(entropies)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Entropy')\n",
        "plt.title('Entropy over Time')\n",
        "plt.savefig(os.path.join(logs_dir_path, \"entropy.png\"))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "#plt.plot(train_step_interval, entropies)\n",
        "plt.plot(entropy_coefs)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Entropy Coef')\n",
        "plt.title('Entropy Coef over Time')\n",
        "plt.savefig(os.path.join(logs_dir_path, \"entropy_coef.png\"))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "#plt.plot(train_step_interval, value_losses)\n",
        "plt.plot(value_losses)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Value Loss')\n",
        "plt.title('Value Loss over Time')\n",
        "plt.savefig(os.path.join(logs_dir_path, \"value_loss.png\"))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "#plt.plot(train_step_interval, policy_losses)\n",
        "plt.plot(policy_losses)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Policy Loss')\n",
        "plt.title('Policy Loss over Time')\n",
        "plt.savefig(os.path.join(logs_dir_path, \"policy_loss.png\"))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(a2c_losses)\n",
        "#plt.plot(train_step_interval, a2c_losses)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('A2C Loss')\n",
        "plt.title('A2C Loss over Time')\n",
        "plt.savefig(os.path.join(logs_dir_path, \"a2c_loss.png\"))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(train_step_interval, head1_entropies, label='Head 1 Entropies')\n",
        "plt.plot(train_step_interval, head2_entropies, label='Head 2 Entropies')\n",
        "plt.plot(train_step_interval, head3_entropies, label='Head 3 Entropies')\n",
        "plt.plot(train_step_interval, head4_entropies, label='Head 4 Entropies')\n",
        "\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Attetnion head Entropy')\n",
        "plt.title('Entropies of Attention Heads Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JZq68NwYofAg"
      },
      "source": [
        "Copy the folders to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDN80Y0kVLow"
      },
      "outputs": [],
      "source": [
        "copy_folders_to_drive(src_directory, \"content/log/\")\n",
        "print(\"Folders have been successfully copied to Google Drive.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ATv74FTHogmC"
      },
      "source": [
        "## Zip Experiment Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxT5LcwacN2N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "def zip_directory(directory_path, output_zip_path):\n",
        "    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(directory_path):\n",
        "            for file in files:\n",
        "                full_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(full_path, directory_path)\n",
        "                zipf.write(full_path, arcname)\n",
        "    print(f\"Directory '{directory_path}' has been successfully zipped to '{output_zip_path}'\")\n",
        "\n",
        "directory_to_zip = '/path/to/directory'\n",
        "output_zip_file = '/path/to/output.zip'\n",
        "\n",
        "zip_directory(src_directory, f\"content/{experiment_name}.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
